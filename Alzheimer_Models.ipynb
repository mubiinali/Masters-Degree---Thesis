{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "colab": {
      "name": "Alzheimer Models.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "35XamKApJSGX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "6fcd19b8-52cf-4e34-e6cc-f170b324db71"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXO6IQrGphZJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "b4cc76c1-1a3c-4e30-fe9f-eab7d4a0f6f4"
      },
      "source": [
        "!pip install tensorflow==1.15"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==1.15 in /usr/local/lib/python3.6/dist-packages (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.9.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.15.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (3.12.4)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.15.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.18.5)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.12.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.34.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.15.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.31.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.8.1)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.2.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.2.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.15) (49.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.2.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15) (2.10.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2KELEAf5gb9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "98f20d08-030f-42e6-c634-d2acb488eab6"
      },
      "source": [
        "import numpy as np\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import ensemble,tree,linear_model\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "from sklearn.preprocessing import OneHotEncoder,MinMaxScaler, StandardScaler\n",
        "from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n",
        "                              GradientBoostingClassifier, ExtraTreesClassifier,GradientBoostingRegressor)\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVR, SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_curve, auc, accuracy_score\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split,cross_val_score\n",
        "# from sklearn.cross_validation import KFold\n",
        "import tensorflow as tf\n",
        "import warnings\n",
        "import plotly.offline as py\n",
        "import plotly.graph_objs as go\n",
        "import plotly.tools as tls\n",
        "py.init_notebook_mode(connected=True)\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "pd.options.display.max_columns=99"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "        <script type=\"text/javascript\">\n",
              "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
              "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
              "        if (typeof require !== 'undefined') {\n",
              "        require.undef(\"plotly\");\n",
              "        requirejs.config({\n",
              "            paths: {\n",
              "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
              "            }\n",
              "        });\n",
              "        require(['plotly'], function(Plotly) {\n",
              "            window._Plotly = Plotly;\n",
              "        });\n",
              "        }\n",
              "        </script>\n",
              "        "
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HEAIi1kJ8Zz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MVvUA-35gcM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "73eb5214-cf2f-4283-a9f9-3016e34b18f2"
      },
      "source": [
        "# Input_Data = pd.read_csv('data/Input_interp_filledCat_codedCat.csv')\n",
        "Input_Data = pd.read_csv('/content/drive/My Drive/NCI /Alzheimer/data/Input_pca_0.95thres.csv')\n",
        "# Input_Data = pd.read_csv('data/Input_remove_corr.csv')\n",
        "# normalize the format of EXAMDATE\n",
        "Input_Data['EXAMDATE'] = pd.to_datetime(Input_Data['EXAMDATE'], errors='coerce')\n",
        "Input_Data.head()\n",
        "# Input_Data['EXAMDATE'].dtype"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>...</th>\n",
              "      <th>232</th>\n",
              "      <th>233</th>\n",
              "      <th>234</th>\n",
              "      <th>235</th>\n",
              "      <th>236</th>\n",
              "      <th>237</th>\n",
              "      <th>238</th>\n",
              "      <th>239</th>\n",
              "      <th>240</th>\n",
              "      <th>241</th>\n",
              "      <th>242</th>\n",
              "      <th>243</th>\n",
              "      <th>244</th>\n",
              "      <th>245</th>\n",
              "      <th>246</th>\n",
              "      <th>247</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>256</th>\n",
              "      <th>257</th>\n",
              "      <th>258</th>\n",
              "      <th>259</th>\n",
              "      <th>260</th>\n",
              "      <th>261</th>\n",
              "      <th>262</th>\n",
              "      <th>263</th>\n",
              "      <th>264</th>\n",
              "      <th>265</th>\n",
              "      <th>266</th>\n",
              "      <th>267</th>\n",
              "      <th>268</th>\n",
              "      <th>269</th>\n",
              "      <th>270</th>\n",
              "      <th>271</th>\n",
              "      <th>272</th>\n",
              "      <th>273</th>\n",
              "      <th>274</th>\n",
              "      <th>275</th>\n",
              "      <th>276</th>\n",
              "      <th>277</th>\n",
              "      <th>PTID_Key</th>\n",
              "      <th>EXAMDATE</th>\n",
              "      <th>DX</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.914627</td>\n",
              "      <td>-0.601258</td>\n",
              "      <td>-1.025954</td>\n",
              "      <td>0.646074</td>\n",
              "      <td>1.011304</td>\n",
              "      <td>0.545207</td>\n",
              "      <td>0.413025</td>\n",
              "      <td>0.157753</td>\n",
              "      <td>0.240323</td>\n",
              "      <td>1.241342</td>\n",
              "      <td>-0.615188</td>\n",
              "      <td>1.990854</td>\n",
              "      <td>0.508145</td>\n",
              "      <td>0.280259</td>\n",
              "      <td>0.153665</td>\n",
              "      <td>-0.378721</td>\n",
              "      <td>1.514711</td>\n",
              "      <td>-0.018951</td>\n",
              "      <td>0.283258</td>\n",
              "      <td>-0.657084</td>\n",
              "      <td>0.221393</td>\n",
              "      <td>0.350555</td>\n",
              "      <td>0.194970</td>\n",
              "      <td>-0.604919</td>\n",
              "      <td>-1.521198</td>\n",
              "      <td>-0.186426</td>\n",
              "      <td>-1.876618</td>\n",
              "      <td>-0.210861</td>\n",
              "      <td>0.673811</td>\n",
              "      <td>-0.596824</td>\n",
              "      <td>0.631972</td>\n",
              "      <td>0.027220</td>\n",
              "      <td>0.872613</td>\n",
              "      <td>-0.775025</td>\n",
              "      <td>0.768025</td>\n",
              "      <td>-1.076237</td>\n",
              "      <td>-0.941686</td>\n",
              "      <td>0.216097</td>\n",
              "      <td>2.490700</td>\n",
              "      <td>-1.265438</td>\n",
              "      <td>-0.277575</td>\n",
              "      <td>-1.353105</td>\n",
              "      <td>-0.388595</td>\n",
              "      <td>0.506031</td>\n",
              "      <td>-0.720210</td>\n",
              "      <td>-0.811336</td>\n",
              "      <td>-0.688113</td>\n",
              "      <td>0.742395</td>\n",
              "      <td>0.775106</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.038844</td>\n",
              "      <td>-1.490284</td>\n",
              "      <td>0.324352</td>\n",
              "      <td>-1.028604</td>\n",
              "      <td>-0.972160</td>\n",
              "      <td>-0.283392</td>\n",
              "      <td>-1.045164</td>\n",
              "      <td>-0.832387</td>\n",
              "      <td>0.798486</td>\n",
              "      <td>0.210059</td>\n",
              "      <td>-0.449635</td>\n",
              "      <td>-0.441003</td>\n",
              "      <td>0.469078</td>\n",
              "      <td>-1.030184</td>\n",
              "      <td>-0.778918</td>\n",
              "      <td>0.415911</td>\n",
              "      <td>0.045974</td>\n",
              "      <td>0.320170</td>\n",
              "      <td>-2.414285</td>\n",
              "      <td>1.074156</td>\n",
              "      <td>2.614145</td>\n",
              "      <td>-1.760806</td>\n",
              "      <td>0.442195</td>\n",
              "      <td>0.431864</td>\n",
              "      <td>1.393142</td>\n",
              "      <td>-1.208740</td>\n",
              "      <td>0.603962</td>\n",
              "      <td>-1.974642</td>\n",
              "      <td>-1.248105</td>\n",
              "      <td>-0.304060</td>\n",
              "      <td>-0.846221</td>\n",
              "      <td>0.430196</td>\n",
              "      <td>0.793928</td>\n",
              "      <td>0.563909</td>\n",
              "      <td>0.624450</td>\n",
              "      <td>-0.352737</td>\n",
              "      <td>0.543500</td>\n",
              "      <td>-1.487216</td>\n",
              "      <td>1.264804</td>\n",
              "      <td>0.964885</td>\n",
              "      <td>-0.080049</td>\n",
              "      <td>1.064061</td>\n",
              "      <td>1.363676</td>\n",
              "      <td>1.058938</td>\n",
              "      <td>0.272191</td>\n",
              "      <td>-0.933477</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2010-12-10</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.551758</td>\n",
              "      <td>-0.979188</td>\n",
              "      <td>-1.117844</td>\n",
              "      <td>1.076659</td>\n",
              "      <td>0.625377</td>\n",
              "      <td>0.593831</td>\n",
              "      <td>0.200446</td>\n",
              "      <td>-0.109782</td>\n",
              "      <td>0.167414</td>\n",
              "      <td>1.085336</td>\n",
              "      <td>-0.908626</td>\n",
              "      <td>0.743372</td>\n",
              "      <td>0.728976</td>\n",
              "      <td>-0.158146</td>\n",
              "      <td>0.144664</td>\n",
              "      <td>0.512072</td>\n",
              "      <td>1.800080</td>\n",
              "      <td>-0.349467</td>\n",
              "      <td>0.228156</td>\n",
              "      <td>-0.650025</td>\n",
              "      <td>0.099217</td>\n",
              "      <td>0.732545</td>\n",
              "      <td>0.032123</td>\n",
              "      <td>-0.570181</td>\n",
              "      <td>-1.853058</td>\n",
              "      <td>0.309348</td>\n",
              "      <td>-1.390368</td>\n",
              "      <td>-0.525124</td>\n",
              "      <td>0.652957</td>\n",
              "      <td>0.002733</td>\n",
              "      <td>1.022216</td>\n",
              "      <td>-0.363548</td>\n",
              "      <td>1.138629</td>\n",
              "      <td>-0.807477</td>\n",
              "      <td>0.886634</td>\n",
              "      <td>-0.933278</td>\n",
              "      <td>-0.718929</td>\n",
              "      <td>-0.177979</td>\n",
              "      <td>1.806847</td>\n",
              "      <td>-0.246725</td>\n",
              "      <td>0.201150</td>\n",
              "      <td>-0.586490</td>\n",
              "      <td>-0.034705</td>\n",
              "      <td>1.524434</td>\n",
              "      <td>-0.490066</td>\n",
              "      <td>-0.777765</td>\n",
              "      <td>-0.901842</td>\n",
              "      <td>0.590897</td>\n",
              "      <td>0.483670</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.341119</td>\n",
              "      <td>-0.416505</td>\n",
              "      <td>-0.392220</td>\n",
              "      <td>-1.099246</td>\n",
              "      <td>-1.542611</td>\n",
              "      <td>0.070696</td>\n",
              "      <td>-0.996193</td>\n",
              "      <td>-0.517035</td>\n",
              "      <td>0.528102</td>\n",
              "      <td>-0.645235</td>\n",
              "      <td>-0.807709</td>\n",
              "      <td>-0.870672</td>\n",
              "      <td>0.461747</td>\n",
              "      <td>-0.857624</td>\n",
              "      <td>-0.120386</td>\n",
              "      <td>0.090665</td>\n",
              "      <td>0.774271</td>\n",
              "      <td>0.565062</td>\n",
              "      <td>-1.341261</td>\n",
              "      <td>1.797199</td>\n",
              "      <td>1.253995</td>\n",
              "      <td>-0.440612</td>\n",
              "      <td>1.865252</td>\n",
              "      <td>-0.874564</td>\n",
              "      <td>1.139276</td>\n",
              "      <td>-1.714373</td>\n",
              "      <td>0.840345</td>\n",
              "      <td>-0.453060</td>\n",
              "      <td>-2.271202</td>\n",
              "      <td>0.701802</td>\n",
              "      <td>-1.159149</td>\n",
              "      <td>0.895165</td>\n",
              "      <td>0.005705</td>\n",
              "      <td>0.684127</td>\n",
              "      <td>0.605223</td>\n",
              "      <td>-0.346341</td>\n",
              "      <td>0.567335</td>\n",
              "      <td>-1.565463</td>\n",
              "      <td>1.082742</td>\n",
              "      <td>2.051711</td>\n",
              "      <td>-0.847921</td>\n",
              "      <td>-0.770229</td>\n",
              "      <td>0.627813</td>\n",
              "      <td>1.707643</td>\n",
              "      <td>-0.733248</td>\n",
              "      <td>-0.978921</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2011-04-07</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.819737</td>\n",
              "      <td>-0.982439</td>\n",
              "      <td>-1.010676</td>\n",
              "      <td>1.049898</td>\n",
              "      <td>0.690065</td>\n",
              "      <td>0.794359</td>\n",
              "      <td>0.281527</td>\n",
              "      <td>-0.584051</td>\n",
              "      <td>0.523014</td>\n",
              "      <td>1.314751</td>\n",
              "      <td>-0.469318</td>\n",
              "      <td>1.015452</td>\n",
              "      <td>0.096584</td>\n",
              "      <td>-0.088802</td>\n",
              "      <td>0.121028</td>\n",
              "      <td>0.640031</td>\n",
              "      <td>1.196030</td>\n",
              "      <td>-0.043073</td>\n",
              "      <td>0.300539</td>\n",
              "      <td>-0.511375</td>\n",
              "      <td>0.033300</td>\n",
              "      <td>0.266959</td>\n",
              "      <td>0.285403</td>\n",
              "      <td>-0.413766</td>\n",
              "      <td>-1.852933</td>\n",
              "      <td>0.159371</td>\n",
              "      <td>-1.505815</td>\n",
              "      <td>-0.257185</td>\n",
              "      <td>1.073447</td>\n",
              "      <td>-0.129415</td>\n",
              "      <td>0.114441</td>\n",
              "      <td>-0.491802</td>\n",
              "      <td>0.351462</td>\n",
              "      <td>-1.344000</td>\n",
              "      <td>0.941796</td>\n",
              "      <td>-1.608389</td>\n",
              "      <td>-0.638489</td>\n",
              "      <td>-0.208845</td>\n",
              "      <td>1.829999</td>\n",
              "      <td>-0.354246</td>\n",
              "      <td>0.138765</td>\n",
              "      <td>-1.168997</td>\n",
              "      <td>0.140994</td>\n",
              "      <td>1.355517</td>\n",
              "      <td>0.127256</td>\n",
              "      <td>-0.306865</td>\n",
              "      <td>-0.525544</td>\n",
              "      <td>0.153632</td>\n",
              "      <td>0.098746</td>\n",
              "      <td>...</td>\n",
              "      <td>-2.057672</td>\n",
              "      <td>-0.952390</td>\n",
              "      <td>0.817166</td>\n",
              "      <td>-0.929542</td>\n",
              "      <td>-1.253907</td>\n",
              "      <td>-0.757082</td>\n",
              "      <td>0.291924</td>\n",
              "      <td>0.301991</td>\n",
              "      <td>-0.853896</td>\n",
              "      <td>-0.014121</td>\n",
              "      <td>0.717416</td>\n",
              "      <td>0.330790</td>\n",
              "      <td>0.103781</td>\n",
              "      <td>-1.087737</td>\n",
              "      <td>0.104273</td>\n",
              "      <td>0.362352</td>\n",
              "      <td>-0.402443</td>\n",
              "      <td>0.103485</td>\n",
              "      <td>-1.247700</td>\n",
              "      <td>-0.150378</td>\n",
              "      <td>0.798621</td>\n",
              "      <td>-0.397531</td>\n",
              "      <td>1.220346</td>\n",
              "      <td>0.753486</td>\n",
              "      <td>0.668390</td>\n",
              "      <td>-0.309202</td>\n",
              "      <td>0.647125</td>\n",
              "      <td>-1.383107</td>\n",
              "      <td>-1.227107</td>\n",
              "      <td>-0.569572</td>\n",
              "      <td>-1.262420</td>\n",
              "      <td>-0.608105</td>\n",
              "      <td>0.859759</td>\n",
              "      <td>0.112846</td>\n",
              "      <td>1.038929</td>\n",
              "      <td>0.008226</td>\n",
              "      <td>0.399049</td>\n",
              "      <td>-0.651663</td>\n",
              "      <td>1.558614</td>\n",
              "      <td>0.498691</td>\n",
              "      <td>-0.013602</td>\n",
              "      <td>0.577223</td>\n",
              "      <td>0.193071</td>\n",
              "      <td>0.872848</td>\n",
              "      <td>0.810032</td>\n",
              "      <td>0.567605</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2011-09-08</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1.336536</td>\n",
              "      <td>0.124959</td>\n",
              "      <td>0.264916</td>\n",
              "      <td>0.702107</td>\n",
              "      <td>-0.807134</td>\n",
              "      <td>0.915510</td>\n",
              "      <td>-0.873423</td>\n",
              "      <td>0.532419</td>\n",
              "      <td>-1.443437</td>\n",
              "      <td>0.450801</td>\n",
              "      <td>-0.561760</td>\n",
              "      <td>-0.045759</td>\n",
              "      <td>0.460943</td>\n",
              "      <td>-0.131770</td>\n",
              "      <td>0.077856</td>\n",
              "      <td>-0.862233</td>\n",
              "      <td>0.352317</td>\n",
              "      <td>-0.057771</td>\n",
              "      <td>-0.283949</td>\n",
              "      <td>-2.005893</td>\n",
              "      <td>-0.729035</td>\n",
              "      <td>-0.121274</td>\n",
              "      <td>0.386319</td>\n",
              "      <td>-0.724438</td>\n",
              "      <td>-0.163793</td>\n",
              "      <td>-0.482719</td>\n",
              "      <td>-0.054568</td>\n",
              "      <td>1.137994</td>\n",
              "      <td>-0.664942</td>\n",
              "      <td>-0.086511</td>\n",
              "      <td>0.020414</td>\n",
              "      <td>-0.143499</td>\n",
              "      <td>0.533670</td>\n",
              "      <td>-0.036038</td>\n",
              "      <td>0.685906</td>\n",
              "      <td>0.058562</td>\n",
              "      <td>-0.216113</td>\n",
              "      <td>-0.077563</td>\n",
              "      <td>0.045318</td>\n",
              "      <td>-0.706913</td>\n",
              "      <td>0.171094</td>\n",
              "      <td>0.184283</td>\n",
              "      <td>0.373552</td>\n",
              "      <td>-0.751905</td>\n",
              "      <td>0.778222</td>\n",
              "      <td>0.892420</td>\n",
              "      <td>-0.362174</td>\n",
              "      <td>-1.056220</td>\n",
              "      <td>0.453497</td>\n",
              "      <td>...</td>\n",
              "      <td>0.744001</td>\n",
              "      <td>-0.413096</td>\n",
              "      <td>-1.867010</td>\n",
              "      <td>-0.117463</td>\n",
              "      <td>-0.523031</td>\n",
              "      <td>1.244548</td>\n",
              "      <td>1.058056</td>\n",
              "      <td>-1.018938</td>\n",
              "      <td>0.014747</td>\n",
              "      <td>-0.899172</td>\n",
              "      <td>0.492322</td>\n",
              "      <td>0.230810</td>\n",
              "      <td>-0.515261</td>\n",
              "      <td>1.091011</td>\n",
              "      <td>-1.302503</td>\n",
              "      <td>-0.515504</td>\n",
              "      <td>-0.167311</td>\n",
              "      <td>-0.036739</td>\n",
              "      <td>0.137325</td>\n",
              "      <td>1.672527</td>\n",
              "      <td>-0.373430</td>\n",
              "      <td>-0.763381</td>\n",
              "      <td>0.547022</td>\n",
              "      <td>-0.613933</td>\n",
              "      <td>1.174471</td>\n",
              "      <td>-1.530746</td>\n",
              "      <td>-0.011245</td>\n",
              "      <td>-0.899292</td>\n",
              "      <td>0.415859</td>\n",
              "      <td>-0.510134</td>\n",
              "      <td>-0.333156</td>\n",
              "      <td>0.464144</td>\n",
              "      <td>1.131625</td>\n",
              "      <td>1.781166</td>\n",
              "      <td>-0.871261</td>\n",
              "      <td>0.403435</td>\n",
              "      <td>-0.576608</td>\n",
              "      <td>0.418030</td>\n",
              "      <td>2.221819</td>\n",
              "      <td>1.403739</td>\n",
              "      <td>0.146228</td>\n",
              "      <td>0.523462</td>\n",
              "      <td>-1.100993</td>\n",
              "      <td>0.755336</td>\n",
              "      <td>1.667155</td>\n",
              "      <td>-0.202909</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2006-07-21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1.562367</td>\n",
              "      <td>0.529278</td>\n",
              "      <td>0.027603</td>\n",
              "      <td>0.916205</td>\n",
              "      <td>-1.045043</td>\n",
              "      <td>1.685051</td>\n",
              "      <td>-0.880973</td>\n",
              "      <td>0.370372</td>\n",
              "      <td>-0.712516</td>\n",
              "      <td>-0.208513</td>\n",
              "      <td>-0.542197</td>\n",
              "      <td>0.410660</td>\n",
              "      <td>0.639716</td>\n",
              "      <td>0.322792</td>\n",
              "      <td>0.323137</td>\n",
              "      <td>-0.525402</td>\n",
              "      <td>0.633719</td>\n",
              "      <td>0.016409</td>\n",
              "      <td>-0.365792</td>\n",
              "      <td>-2.154638</td>\n",
              "      <td>-0.988543</td>\n",
              "      <td>-0.190350</td>\n",
              "      <td>-0.110402</td>\n",
              "      <td>-0.890922</td>\n",
              "      <td>0.632547</td>\n",
              "      <td>-0.196063</td>\n",
              "      <td>-0.280631</td>\n",
              "      <td>1.134458</td>\n",
              "      <td>-1.075078</td>\n",
              "      <td>-0.568868</td>\n",
              "      <td>1.025541</td>\n",
              "      <td>-0.529108</td>\n",
              "      <td>0.440324</td>\n",
              "      <td>-0.059409</td>\n",
              "      <td>0.778075</td>\n",
              "      <td>-0.265290</td>\n",
              "      <td>-0.385762</td>\n",
              "      <td>-0.359963</td>\n",
              "      <td>-0.196115</td>\n",
              "      <td>0.134002</td>\n",
              "      <td>-0.667245</td>\n",
              "      <td>0.960308</td>\n",
              "      <td>0.485088</td>\n",
              "      <td>0.152973</td>\n",
              "      <td>0.522267</td>\n",
              "      <td>0.608669</td>\n",
              "      <td>-0.847921</td>\n",
              "      <td>-0.792681</td>\n",
              "      <td>0.028274</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.864151</td>\n",
              "      <td>1.162983</td>\n",
              "      <td>-3.350067</td>\n",
              "      <td>0.056246</td>\n",
              "      <td>-0.226370</td>\n",
              "      <td>0.910181</td>\n",
              "      <td>0.605163</td>\n",
              "      <td>-2.240222</td>\n",
              "      <td>1.565027</td>\n",
              "      <td>-0.573346</td>\n",
              "      <td>0.343262</td>\n",
              "      <td>-0.023751</td>\n",
              "      <td>-2.147292</td>\n",
              "      <td>0.479222</td>\n",
              "      <td>-0.539234</td>\n",
              "      <td>-0.522955</td>\n",
              "      <td>-0.138616</td>\n",
              "      <td>0.087464</td>\n",
              "      <td>-1.196574</td>\n",
              "      <td>1.617743</td>\n",
              "      <td>-0.340345</td>\n",
              "      <td>-0.801156</td>\n",
              "      <td>-0.878616</td>\n",
              "      <td>-1.669274</td>\n",
              "      <td>0.025889</td>\n",
              "      <td>-1.299914</td>\n",
              "      <td>-0.239468</td>\n",
              "      <td>-0.392099</td>\n",
              "      <td>1.173756</td>\n",
              "      <td>0.437082</td>\n",
              "      <td>-1.063262</td>\n",
              "      <td>0.881679</td>\n",
              "      <td>0.625876</td>\n",
              "      <td>0.668160</td>\n",
              "      <td>-0.368181</td>\n",
              "      <td>0.118717</td>\n",
              "      <td>-0.841962</td>\n",
              "      <td>-0.389618</td>\n",
              "      <td>3.025075</td>\n",
              "      <td>0.330001</td>\n",
              "      <td>-0.155738</td>\n",
              "      <td>1.772588</td>\n",
              "      <td>-0.456464</td>\n",
              "      <td>1.188597</td>\n",
              "      <td>0.326834</td>\n",
              "      <td>-1.148805</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2007-01-16</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 281 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          0         1         2         3         4         5         6  \\\n",
              "0 -0.914627 -0.601258 -1.025954  0.646074  1.011304  0.545207  0.413025   \n",
              "1 -0.551758 -0.979188 -1.117844  1.076659  0.625377  0.593831  0.200446   \n",
              "2 -0.819737 -0.982439 -1.010676  1.049898  0.690065  0.794359  0.281527   \n",
              "3 -1.336536  0.124959  0.264916  0.702107 -0.807134  0.915510 -0.873423   \n",
              "4 -1.562367  0.529278  0.027603  0.916205 -1.045043  1.685051 -0.880973   \n",
              "\n",
              "          7         8         9        10        11        12        13  \\\n",
              "0  0.157753  0.240323  1.241342 -0.615188  1.990854  0.508145  0.280259   \n",
              "1 -0.109782  0.167414  1.085336 -0.908626  0.743372  0.728976 -0.158146   \n",
              "2 -0.584051  0.523014  1.314751 -0.469318  1.015452  0.096584 -0.088802   \n",
              "3  0.532419 -1.443437  0.450801 -0.561760 -0.045759  0.460943 -0.131770   \n",
              "4  0.370372 -0.712516 -0.208513 -0.542197  0.410660  0.639716  0.322792   \n",
              "\n",
              "         14        15        16        17        18        19        20  \\\n",
              "0  0.153665 -0.378721  1.514711 -0.018951  0.283258 -0.657084  0.221393   \n",
              "1  0.144664  0.512072  1.800080 -0.349467  0.228156 -0.650025  0.099217   \n",
              "2  0.121028  0.640031  1.196030 -0.043073  0.300539 -0.511375  0.033300   \n",
              "3  0.077856 -0.862233  0.352317 -0.057771 -0.283949 -2.005893 -0.729035   \n",
              "4  0.323137 -0.525402  0.633719  0.016409 -0.365792 -2.154638 -0.988543   \n",
              "\n",
              "         21        22        23        24        25        26        27  \\\n",
              "0  0.350555  0.194970 -0.604919 -1.521198 -0.186426 -1.876618 -0.210861   \n",
              "1  0.732545  0.032123 -0.570181 -1.853058  0.309348 -1.390368 -0.525124   \n",
              "2  0.266959  0.285403 -0.413766 -1.852933  0.159371 -1.505815 -0.257185   \n",
              "3 -0.121274  0.386319 -0.724438 -0.163793 -0.482719 -0.054568  1.137994   \n",
              "4 -0.190350 -0.110402 -0.890922  0.632547 -0.196063 -0.280631  1.134458   \n",
              "\n",
              "         28        29        30        31        32        33        34  \\\n",
              "0  0.673811 -0.596824  0.631972  0.027220  0.872613 -0.775025  0.768025   \n",
              "1  0.652957  0.002733  1.022216 -0.363548  1.138629 -0.807477  0.886634   \n",
              "2  1.073447 -0.129415  0.114441 -0.491802  0.351462 -1.344000  0.941796   \n",
              "3 -0.664942 -0.086511  0.020414 -0.143499  0.533670 -0.036038  0.685906   \n",
              "4 -1.075078 -0.568868  1.025541 -0.529108  0.440324 -0.059409  0.778075   \n",
              "\n",
              "         35        36        37        38        39        40        41  \\\n",
              "0 -1.076237 -0.941686  0.216097  2.490700 -1.265438 -0.277575 -1.353105   \n",
              "1 -0.933278 -0.718929 -0.177979  1.806847 -0.246725  0.201150 -0.586490   \n",
              "2 -1.608389 -0.638489 -0.208845  1.829999 -0.354246  0.138765 -1.168997   \n",
              "3  0.058562 -0.216113 -0.077563  0.045318 -0.706913  0.171094  0.184283   \n",
              "4 -0.265290 -0.385762 -0.359963 -0.196115  0.134002 -0.667245  0.960308   \n",
              "\n",
              "         42        43        44        45        46        47        48  ...  \\\n",
              "0 -0.388595  0.506031 -0.720210 -0.811336 -0.688113  0.742395  0.775106  ...   \n",
              "1 -0.034705  1.524434 -0.490066 -0.777765 -0.901842  0.590897  0.483670  ...   \n",
              "2  0.140994  1.355517  0.127256 -0.306865 -0.525544  0.153632  0.098746  ...   \n",
              "3  0.373552 -0.751905  0.778222  0.892420 -0.362174 -1.056220  0.453497  ...   \n",
              "4  0.485088  0.152973  0.522267  0.608669 -0.847921 -0.792681  0.028274  ...   \n",
              "\n",
              "        232       233       234       235       236       237       238  \\\n",
              "0 -1.038844 -1.490284  0.324352 -1.028604 -0.972160 -0.283392 -1.045164   \n",
              "1 -1.341119 -0.416505 -0.392220 -1.099246 -1.542611  0.070696 -0.996193   \n",
              "2 -2.057672 -0.952390  0.817166 -0.929542 -1.253907 -0.757082  0.291924   \n",
              "3  0.744001 -0.413096 -1.867010 -0.117463 -0.523031  1.244548  1.058056   \n",
              "4 -0.864151  1.162983 -3.350067  0.056246 -0.226370  0.910181  0.605163   \n",
              "\n",
              "        239       240       241       242       243       244       245  \\\n",
              "0 -0.832387  0.798486  0.210059 -0.449635 -0.441003  0.469078 -1.030184   \n",
              "1 -0.517035  0.528102 -0.645235 -0.807709 -0.870672  0.461747 -0.857624   \n",
              "2  0.301991 -0.853896 -0.014121  0.717416  0.330790  0.103781 -1.087737   \n",
              "3 -1.018938  0.014747 -0.899172  0.492322  0.230810 -0.515261  1.091011   \n",
              "4 -2.240222  1.565027 -0.573346  0.343262 -0.023751 -2.147292  0.479222   \n",
              "\n",
              "        246       247       248       249       250       251       252  \\\n",
              "0 -0.778918  0.415911  0.045974  0.320170 -2.414285  1.074156  2.614145   \n",
              "1 -0.120386  0.090665  0.774271  0.565062 -1.341261  1.797199  1.253995   \n",
              "2  0.104273  0.362352 -0.402443  0.103485 -1.247700 -0.150378  0.798621   \n",
              "3 -1.302503 -0.515504 -0.167311 -0.036739  0.137325  1.672527 -0.373430   \n",
              "4 -0.539234 -0.522955 -0.138616  0.087464 -1.196574  1.617743 -0.340345   \n",
              "\n",
              "        253       254       255       256       257       258       259  \\\n",
              "0 -1.760806  0.442195  0.431864  1.393142 -1.208740  0.603962 -1.974642   \n",
              "1 -0.440612  1.865252 -0.874564  1.139276 -1.714373  0.840345 -0.453060   \n",
              "2 -0.397531  1.220346  0.753486  0.668390 -0.309202  0.647125 -1.383107   \n",
              "3 -0.763381  0.547022 -0.613933  1.174471 -1.530746 -0.011245 -0.899292   \n",
              "4 -0.801156 -0.878616 -1.669274  0.025889 -1.299914 -0.239468 -0.392099   \n",
              "\n",
              "        260       261       262       263       264       265       266  \\\n",
              "0 -1.248105 -0.304060 -0.846221  0.430196  0.793928  0.563909  0.624450   \n",
              "1 -2.271202  0.701802 -1.159149  0.895165  0.005705  0.684127  0.605223   \n",
              "2 -1.227107 -0.569572 -1.262420 -0.608105  0.859759  0.112846  1.038929   \n",
              "3  0.415859 -0.510134 -0.333156  0.464144  1.131625  1.781166 -0.871261   \n",
              "4  1.173756  0.437082 -1.063262  0.881679  0.625876  0.668160 -0.368181   \n",
              "\n",
              "        267       268       269       270       271       272       273  \\\n",
              "0 -0.352737  0.543500 -1.487216  1.264804  0.964885 -0.080049  1.064061   \n",
              "1 -0.346341  0.567335 -1.565463  1.082742  2.051711 -0.847921 -0.770229   \n",
              "2  0.008226  0.399049 -0.651663  1.558614  0.498691 -0.013602  0.577223   \n",
              "3  0.403435 -0.576608  0.418030  2.221819  1.403739  0.146228  0.523462   \n",
              "4  0.118717 -0.841962 -0.389618  3.025075  0.330001 -0.155738  1.772588   \n",
              "\n",
              "        274       275       276       277  PTID_Key   EXAMDATE  DX  \n",
              "0  1.363676  1.058938  0.272191 -0.933477       1.0 2010-12-10   2  \n",
              "1  0.627813  1.707643 -0.733248 -0.978921       1.0 2011-04-07   2  \n",
              "2  0.193071  0.872848  0.810032  0.567605       1.0 2011-09-08   2  \n",
              "3 -1.100993  0.755336  1.667155 -0.202909       2.0 2006-07-21   0  \n",
              "4 -0.456464  1.188597  0.326834 -1.148805       2.0 2007-01-16   0  \n",
              "\n",
              "[5 rows x 281 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWo_Wjgc5gdA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d0aac376-83e1-4f3a-87df-de55f8e22414"
      },
      "source": [
        "Input_Data.shape"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8715, 281)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtj4qQLO5gdP",
        "colab_type": "text"
      },
      "source": [
        "### training target"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mpxvqw6X5gdR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Do linear interpolation if there is data before and after for the same object, then do ffil and bfil\n",
        "def linear_interp(data):\n",
        "    ID_list = np.unique(data.PTID_Key.values)\n",
        "    # Create an empty dataframe with all columns from data\n",
        "    Input_new=pd.DataFrame(columns=data.columns)\n",
        "#     print(ID_list)\n",
        "    for ID in ID_list:\n",
        "#         print(ID)\n",
        "        df=data[data['PTID_Key']==ID]\n",
        "        # interpolate only for numeric data\n",
        "        df=df.interpolate()\n",
        "        # ffill, bfill numeric data that can't be interpolate as well as categorical data\n",
        "        df=df.fillna(method='ffill')\n",
        "        df=df.fillna(method='bfill')\n",
        "        Input_new=pd.concat([Input_new, df], ignore_index=True)\n",
        "\n",
        "    print(\"Remaining missing values: \", Input_new.isnull().sum().sum() )\n",
        "    print(\"Filled percentage: \", (1- Input_new.isnull().sum().sum()/data.isnull().sum().sum())*100,\"%\")\n",
        "    return Input_new"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OAq_pxb5gdb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "def scale(data):\n",
        "#     df=pd.DataFrame(columns=data.columns)\n",
        "#     print(df.shape)\n",
        "#     df=data.drop(['PTID_Key','EXAMDATE', 'EXAMDATE_bl'], axis=1)\n",
        "    features=data.select_dtypes(exclude=[\"datetime\"]).columns.tolist()\n",
        "    # Separating out the features\n",
        "    for feature in features:\n",
        "    # Standardizing the features\n",
        "#         print(feature)\n",
        "        data[feature] = StandardScaler().fit_transform(data[feature].values.reshape(-1,1))\n",
        "    # df = StandardScaler().fit_transform(df)\n",
        "    return data\n",
        "    # df.isnull().sum().sum()"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHLiffVR5gdl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the date format, check if there are objects in output but not in input, sort by PTID and date, then linear interpolate\n",
        "def output_prep(data):    \n",
        "    \n",
        "#     data.rename(index=str,columns={'Date':'EXAMDATE'},inplace=True)\n",
        "    # normalize the format of EXAMDATE\n",
        "    data['Date'] = pd.to_datetime(data['Date'])\n",
        "    data.head()\n",
        "    \n",
        "    # We can see if there is objects in data but not in df_data\n",
        "    data_id = list(set(data['PTID_Key'].values))\n",
        "    IDlist=[]\n",
        "    for ID in data_id:\n",
        "        if ID not in Input_Data['PTID_Key'].values:\n",
        "            IDlist.append(ID)\n",
        "    if len(IDlist)==0:\n",
        "        print(\"All objects in the train target file are in input data file\")\n",
        "    else:\n",
        "        print(\"% objects in the train target file are not in input data file:\" % (len(IDlist)),IDlist)\n",
        "\n",
        "    # sort the data by DX_bl and Month, then fill the missing data\n",
        "    data = data.sort_values(by=['PTID_Key','Date'])\n",
        "    data = data.reset_index(drop=True)\n",
        "\n",
        "    data_new=linear_interp(data)\n",
        "    \n",
        "    # Drop the rows where at least one element is missing.\n",
        "    data_new=data_new.dropna()\n",
        "    data_new = data_new.reset_index(drop=True)\n",
        "    print(\"After dropping missing values, the shape of the dataset is: \", data_new.shape)\n",
        "    print(\"After dropping missing values, the total number of missing values is: \", data_new.isnull().sum().sum())\n",
        "#     print(data_new.head())\n",
        "    return data_new"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pX1vq2yW5gdu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the input and output data for feeding models: cross-product, remove PTID & date, then scale data\n",
        "def prep_for_models(Input,Output):\n",
        "#     Input=scale(Input)\n",
        "    # Cross-product (based on same PTID) the input (M rows) and output files (N rows) to get the data (MxN) for models\n",
        "    data=Input.merge(Output, left_on='PTID_Key', right_on='PTID_Key')\n",
        "    print(\"After merging input and output, the shape of the data is: \",data.shape)\n",
        "    \n",
        "    # Get the month between each input and each output\n",
        "    data['Month_inter']=np.ceil((data['Date']-data['EXAMDATE'])/np.timedelta64(1, 'M'))\n",
        "    print(\"Adding the month interval, the shape of the data is: \",data.shape)\n",
        "    # Scale numeric data\n",
        "    data = data.drop(\"PTID_Key\", axis=1) \n",
        "    data=data.select_dtypes(exclude=['object','datetime'])\n",
        "    print(\"Removing PTID_Key and Date columns, the shape of the data is: \",data.shape) \n",
        "    \n",
        "#     print(data_input.head())\n",
        "    \n",
        "    return data"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVakNsQ45gd7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "115802cf-c505-4ae2-926d-6aa2afbc7642"
      },
      "source": [
        "train = pd.read_csv('/content/drive/My Drive/NCI /Alzheimer/data/TADPOLE_TargetData_train.csv')\n",
        "train.head()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>PTID_Key</th>\n",
              "      <th>CN_Diag</th>\n",
              "      <th>MCI_Diag</th>\n",
              "      <th>AD_Diag</th>\n",
              "      <th>ADAS13</th>\n",
              "      <th>Ventricles_Norm</th>\n",
              "      <th>MMSE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7/10/13</td>\n",
              "      <td>785</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.012737</td>\n",
              "      <td>28.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1/15/13</td>\n",
              "      <td>785</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1/17/14</td>\n",
              "      <td>785</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7/29/14</td>\n",
              "      <td>785</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>30.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7/24/15</td>\n",
              "      <td>785</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0.013934</td>\n",
              "      <td>25.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Date  PTID_Key  CN_Diag  MCI_Diag  AD_Diag  ADAS13  Ventricles_Norm  \\\n",
              "0  7/10/13       785      0.0       1.0      0.0     5.0         0.012737   \n",
              "1  1/15/13       785      NaN       NaN      NaN     NaN              NaN   \n",
              "2  1/17/14       785      NaN       NaN      NaN     NaN              NaN   \n",
              "3  7/29/14       785      1.0       0.0      0.0     7.0              NaN   \n",
              "4  7/24/15       785      1.0       0.0      0.0    11.0         0.013934   \n",
              "\n",
              "   MMSE  \n",
              "0  28.0  \n",
              "1   NaN  \n",
              "2   NaN  \n",
              "3  30.0  \n",
              "4  25.0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nz6i7mqT5geJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cd16215e-633a-444b-e13f-f43586db4d47"
      },
      "source": [
        "train.shape"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2506, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWUEOdhy5geT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "626ca67a-3c51-4073-f54f-fbdc466a35db"
      },
      "source": [
        "val=pd.read_csv('/content/drive/My Drive/NCI /Alzheimer/data/TADPOLE_TargetData_validation.csv')\n",
        "val.head()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>PTID_Key</th>\n",
              "      <th>CN_Diag</th>\n",
              "      <th>MCI_Diag</th>\n",
              "      <th>AD_Diag</th>\n",
              "      <th>ADAS13</th>\n",
              "      <th>Ventricles_Norm</th>\n",
              "      <th>MMSE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2014-01-02</td>\n",
              "      <td>1603</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2015-01-15</td>\n",
              "      <td>1603</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2014-10-29</td>\n",
              "      <td>1603</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2015-08-11</td>\n",
              "      <td>1603</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2016-07-18</td>\n",
              "      <td>1603</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Date  PTID_Key  CN_Diag  MCI_Diag  AD_Diag  ADAS13  Ventricles_Norm  \\\n",
              "0  2014-01-02      1603      NaN       NaN      NaN     NaN              NaN   \n",
              "1  2015-01-15      1603      NaN       NaN      NaN     NaN              NaN   \n",
              "2  2014-10-29      1603      NaN       NaN      NaN     NaN              NaN   \n",
              "3  2015-08-11      1603      NaN       NaN      NaN     NaN              NaN   \n",
              "4  2016-07-18      1603      NaN       NaN      NaN     NaN              NaN   \n",
              "\n",
              "   MMSE  \n",
              "0   NaN  \n",
              "1   NaN  \n",
              "2   NaN  \n",
              "3   NaN  \n",
              "4   NaN  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93kg2WMJ5gee",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "85f3ca0f-5bf4-4b7a-c171-6f8825368814"
      },
      "source": [
        "test=pd.read_csv('/content/drive/My Drive/NCI /Alzheimer/data/TADPOLE_PredictTargetData_test.csv')\n",
        "test.head()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>PTID_Key</th>\n",
              "      <th>CN_Diag</th>\n",
              "      <th>MCI_Diag</th>\n",
              "      <th>AD_Diag</th>\n",
              "      <th>ADAS13</th>\n",
              "      <th>Ventricles_Norm</th>\n",
              "      <th>MMSE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1/25/13</td>\n",
              "      <td>583</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6/25/13</td>\n",
              "      <td>583</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3/10/14</td>\n",
              "      <td>583</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>12/4/13</td>\n",
              "      <td>809</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4/25/13</td>\n",
              "      <td>809</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Date  PTID_Key  CN_Diag  MCI_Diag  AD_Diag  ADAS13  Ventricles_Norm  \\\n",
              "0  1/25/13       583      NaN       NaN      NaN     NaN              NaN   \n",
              "1  6/25/13       583      NaN       NaN      NaN     NaN              NaN   \n",
              "2  3/10/14       583      NaN       NaN      NaN     NaN              NaN   \n",
              "3  12/4/13       809      NaN       NaN      NaN     NaN              NaN   \n",
              "4  4/25/13       809      NaN       NaN      NaN     NaN              NaN   \n",
              "\n",
              "   MMSE  \n",
              "0   NaN  \n",
              "1   NaN  \n",
              "2   NaN  \n",
              "3   NaN  \n",
              "4   NaN  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKoqEOoy5gep",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "99bd4d99-b934-449d-d69c-8e27480ddd9a"
      },
      "source": [
        "Input_Data['EXAMDATE'].dtype\n",
        "#train_proc['Date'].dtype\n",
        "#train_proc.head()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dtype('<M8[ns]')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUCKaB795gex",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "19489fca-a12d-4a59-fb89-fb40e5da12a7"
      },
      "source": [
        "# Prepare training data\n",
        "train_proc=output_prep(train)\n",
        "train_proc.to_csv('/content/drive/My Drive/NCI /Alzheimer/data/train_preprocessed.csv',index=False)\n",
        "\n",
        "# Prepare validation data\n",
        "val_proc=output_prep(val)\n",
        "val_proc.to_csv('/content/drive/My Drive/NCI /Alzheimer/data/val_preprocessed.csv',index=False)\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All objects in the train target file are in input data file\n",
            "Remaining missing values:  1037\n",
            "Filled percentage:  82.67624457066488 %\n",
            "After dropping missing values, the shape of the dataset is:  (1901, 8)\n",
            "After dropping missing values, the total number of missing values is:  0\n",
            "All objects in the train target file are in input data file\n",
            "Remaining missing values:  408\n",
            "Filled percentage:  80.17492711370262 %\n",
            "After dropping missing values, the shape of the dataset is:  (639, 8)\n",
            "After dropping missing values, the total number of missing values is:  0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YelViJS5ge6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "19910de6-a517-4c38-ebde-6da151dddbb9"
      },
      "source": [
        "# Get x,y for training data\n",
        "data_con = ['ADAS13','Ventricles_Norm','MMSE']\n",
        "data_cat=['CN_Diag','MCI_Diag','AD_Diag']\n",
        "\n",
        "train_final=prep_for_models(Input_Data,train_proc)\n",
        "\n",
        "x_train=train_final.drop(data_con,axis=1).drop(data_cat,axis=1)\n",
        "\n",
        "y_train_adas13=train_final['ADAS13']\n",
        "y_train_ventricles=train_final['Ventricles_Norm']\n",
        "y_train_mmse=train_final['MMSE']\n",
        "\n",
        "# Diagnosis\n",
        "y_train_diag = train_final[['CN_Diag','MCI_Diag','AD_Diag']]\n",
        "\n",
        "y_train_diag['CN_Diag'] = y_train_diag['CN_Diag'].astype('int')\n",
        "y_train_diag['MCI_Diag'] = y_train_diag['MCI_Diag'].astype('int')\n",
        "y_train_diag['AD_Diag'] = y_train_diag['AD_Diag'].astype('int')\n",
        "\n",
        "# Encode one-hot encoding back to label encoding (0: CN_Diag, 1: MCI_Diag, 2: AD_Diag)\n",
        "y_train_diag['Diag'] = np.argmax(y_train_diag[['CN_Diag','MCI_Diag','AD_Diag']].values,axis=1)\n",
        "\n",
        "y_train_diag1 = y_train_diag['Diag']\n",
        "y_train_diag2 = y_train_diag[['CN_Diag','MCI_Diag','AD_Diag']]\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "After merging input and output, the shape of the data is:  (6716, 288)\n",
            "Adding the month interval, the shape of the data is:  (6716, 289)\n",
            "Removing PTID_Key and Date columns, the shape of the data is:  (6716, 286)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ccjeR2h5gfC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "b10c333a-b1fb-46fc-e694-21947dfe1b15"
      },
      "source": [
        "# Get x,y for validation data\n",
        "data_con = ['ADAS13','Ventricles_Norm','MMSE']\n",
        "data_cat=['CN_Diag','MCI_Diag','AD_Diag']\n",
        "\n",
        "val_final=prep_for_models(Input_Data,val_proc)\n",
        "\n",
        "x_val=val_final.drop(data_con,axis=1).drop(data_cat,axis=1)\n",
        "\n",
        "y_val_adas13=val_final['ADAS13']\n",
        "y_val_ventricles=val_final['Ventricles_Norm']\n",
        "y_val_mmse=val_final['MMSE']\n",
        "\n",
        "# Diagnosis\n",
        "y_val_diag = val_final[['CN_Diag','MCI_Diag','AD_Diag']]\n",
        "\n",
        "y_val_diag['CN_Diag'] = y_val_diag['CN_Diag'].astype('int')\n",
        "y_val_diag['MCI_Diag'] = y_val_diag['MCI_Diag'].astype('int')\n",
        "y_val_diag['AD_Diag'] = y_val_diag['AD_Diag'].astype('int')\n",
        "\n",
        "# Encode one-hot encoding back to label encoding (0: CN_Diag, 1: MCI_Diag, 2: AD_Diag)\n",
        "y_val_diag['Diag'] = np.argmax(y_val_diag[['CN_Diag','MCI_Diag','AD_Diag']].values,axis=1)\n",
        "\n",
        "y_val_diag1 = y_val_diag['Diag']\n",
        "y_val_diag2 = y_val_diag[['CN_Diag','MCI_Diag','AD_Diag']]\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "After merging input and output, the shape of the data is:  (2238, 288)\n",
            "Adding the month interval, the shape of the data is:  (2238, 289)\n",
            "Removing PTID_Key and Date columns, the shape of the data is:  (2238, 286)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwyoOheQ5gfL",
        "colab_type": "text"
      },
      "source": [
        "## Machine Learning Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QE6-8_9U5gfN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Models\n",
        "\n",
        "from sklearn import ensemble,tree,linear_model\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "\n",
        "# Print R2 and RMSE scores\n",
        "def get_score(prediction, labels):\n",
        "    print('R2: {}'.format(r2_score(prediction, labels)))\n",
        "    print('RMSE: {}'.format(np.sqrt(mean_squared_error(prediction,labels))))"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svuhainS5gfS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Show scores for train and validation sets\n",
        "def train_test(estimator, x_train, x_test, y_train, y_test):\n",
        "    prediction_train = estimator.predict(x_train)\n",
        "    print(estimator)\n",
        "    get_score(prediction_train,y_train)\n",
        "    \n",
        "    prediction_test = estimator.predict(x_test)\n",
        "    print('Test')\n",
        "    get_score(prediction_test, y_test)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiWoHPyA4yPf",
        "colab_type": "text"
      },
      "source": [
        "###Elastic Net Regressor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2J5lAi35gfb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Elastic Net\n",
        "def eNet(x_train,y_train,x_val,y_val):\n",
        "    ENSTest = linear_model.ElasticNetCV(alphas=[  ],l1_ratio=[.01, .1, .5, .9, .99], max_iter=5000).fit(x_train,y_train)\n",
        "    train_test(ENSTest, x_train,x_val,y_train,y_val)\n",
        "\n",
        "    # Average R2 score and standard deviation of 5-fold cross-validation\n",
        "    scores = cross_val_score(ENSTest, x_train, y_train, cv=5)\n",
        "    print('Accuracy: %0.2f (+/- %0.2f)' % (scores.mean(), scores.std()*2))"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dx_Zp_uH5gfh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "outputId": "3dbb622b-88be-435e-efcf-83daae016b04"
      },
      "source": [
        "print(\"ADAS13\")\n",
        "eNet(x_train,y_train_adas13,x_val,y_val_adas13)\n",
        "print(\"Ventricles_Norm\")\n",
        "eNet(x_train,y_train_ventricles,x_val,y_val_ventricles)\n",
        "print(\"MMSE\")\n",
        "eNet(x_train,y_train_mmse,x_val,y_val_mmse)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ADAS13\n",
            "ElasticNetCV(alphas=[0.0001, 0.0005, 0.001, 0.01, 0.1, 1, 10], copy_X=True,\n",
            "             cv=None, eps=0.001, fit_intercept=True,\n",
            "             l1_ratio=[0.01, 0.1, 0.5, 0.9, 0.99], max_iter=5000, n_alphas=100,\n",
            "             n_jobs=None, normalize=False, positive=False, precompute='auto',\n",
            "             random_state=None, selection='cyclic', tol=0.0001, verbose=0)\n",
            "R2: 0.7637561972169721\n",
            "RMSE: 4.761452397771426\n",
            "Test\n",
            "R2: 0.1272502499490319\n",
            "RMSE: 7.763820055598533\n",
            "Accuracy: 0.58 (+/- 0.22)\n",
            "Ventricles_Norm\n",
            "ElasticNetCV(alphas=[0.0001, 0.0005, 0.001, 0.01, 0.1, 1, 10], copy_X=True,\n",
            "             cv=None, eps=0.001, fit_intercept=True,\n",
            "             l1_ratio=[0.01, 0.1, 0.5, 0.9, 0.99], max_iter=5000, n_alphas=100,\n",
            "             n_jobs=None, normalize=False, positive=False, precompute='auto',\n",
            "             random_state=None, selection='cyclic', tol=0.0001, verbose=0)\n",
            "R2: 0.9450985570370511\n",
            "RMSE: 0.002630125951292067\n",
            "Test\n",
            "R2: 0.8349707159800148\n",
            "RMSE: 0.00473296995855007\n",
            "Accuracy: 0.80 (+/- 0.07)\n",
            "MMSE\n",
            "ElasticNetCV(alphas=[0.0001, 0.0005, 0.001, 0.01, 0.1, 1, 10], copy_X=True,\n",
            "             cv=None, eps=0.001, fit_intercept=True,\n",
            "             l1_ratio=[0.01, 0.1, 0.5, 0.9, 0.99], max_iter=5000, n_alphas=100,\n",
            "             n_jobs=None, normalize=False, positive=False, precompute='auto',\n",
            "             random_state=None, selection='cyclic', tol=0.0001, verbose=0)\n",
            "R2: 0.28931836752440665\n",
            "RMSE: 2.2196518883612084\n",
            "Test\n",
            "R2: -0.8365894360453778\n",
            "RMSE: 2.8224350304591455\n",
            "Accuracy: 0.43 (+/- 0.17)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0ON6vOT4q_-",
        "colab_type": "text"
      },
      "source": [
        "###Support Vector Regressors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pP1d0Lf5gfo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def svr(x_train,y_train,x_val,y_val):    \n",
        "    svr = SVR(gamma='scale',C=1.0,epsilon=0.2).fit(x_train,y_train)\n",
        "    train_test(svr, x_train,x_val,y_train,y_val)\n",
        "\n",
        "    # Average R2 score and standard deviation of 5-fold cross-validation\n",
        "    scores = cross_val_score(svr, x_train, y_train, cv=5)\n",
        "    print('Accuracy: %0.2f (+/- %0.2f)' % (scores.mean(), scores.std()*2))"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfJNYC5F5gft",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "832c68f8-b5a4-47c1-ea64-0ead745fb754"
      },
      "source": [
        "print(\"ADAS13\")\n",
        "svr(x_train,y_train_adas13,x_val,y_val_adas13)\n",
        "print(\"Ventricles_Norm\")\n",
        "svr(x_train,y_train_ventricles,x_val,y_val_ventricles)\n",
        "print(\"MMSE\")\n",
        "svr(x_train,y_train_mmse,x_val,y_val_mmse)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ADAS13\n",
            "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.2, gamma='scale',\n",
            "    kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
            "R2: -3.2291382038888186\n",
            "RMSE: 8.860751651893896\n",
            "Test\n",
            "R2: -13.462647793908443\n",
            "RMSE: 10.635230819009879\n",
            "Accuracy: 0.14 (+/- 0.06)\n",
            "Ventricles_Norm\n",
            "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.2, gamma='scale',\n",
            "    kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
            "R2: -7.747377135104545e+30\n",
            "RMSE: 0.019313794043997957\n",
            "Test\n",
            "R2: 0.0\n",
            "RMSE: 0.019544580340634176\n",
            "Accuracy: -1.82 (+/- 2.10)\n",
            "MMSE\n",
            "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.2, gamma='scale',\n",
            "    kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
            "R2: -0.33872547044090706\n",
            "RMSE: 2.4399140180309162\n",
            "Test\n",
            "R2: -3.6935750303711927\n",
            "RMSE: 3.137229627277827\n",
            "Accuracy: 0.20 (+/- 0.09)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzk4172B4kxl",
        "colab_type": "text"
      },
      "source": [
        "###Gradient Boosting for regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBrLb6Ta5gfw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Gradient Boosting\n",
        "def gBoost(x_train,y_train,x_val,y_val):\n",
        "    GBest = ensemble.GradientBoostingRegressor(n_estimators=3000,learning_rate=0.05,max_depth=3,max_features='sqrt',\n",
        "                                              min_samples_leaf=15,min_samples_split=10,loss='huber').fit(x_train,y_train)\n",
        "    train_test(GBest,x_train,x_train,y_train,y_train)\n",
        "    # Average R2 score and standard deviation of 5-fold cross-validation\n",
        "    scores = cross_val_score(GBest, x_train, y_train, cv=5)\n",
        "    print('Accuracy: %0.2f (+/- %0.2f)' % (scores.mean(), scores.std()*2))\n",
        "\n",
        "    x_train.head()"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "igTTJZd95gf0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "outputId": "c6f4550b-27b0-4fc6-bd23-801be807ebeb"
      },
      "source": [
        "print(\"ADAS13\")\n",
        "gBoost(x_train,y_train_adas13,x_val,y_val_adas13)\n",
        "print(\"Ventricles_Norm\")\n",
        "gBoost(x_train,y_train_ventricles,x_val,y_val_ventricles)\n",
        "print(\"MMSE\")\n",
        "gBoost(x_train,y_train_mmse,x_val,y_val_mmse)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ADAS13\n",
            "GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse',\n",
            "                          init=None, learning_rate=0.05, loss='huber',\n",
            "                          max_depth=3, max_features='sqrt', max_leaf_nodes=None,\n",
            "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                          min_samples_leaf=15, min_samples_split=10,\n",
            "                          min_weight_fraction_leaf=0.0, n_estimators=3000,\n",
            "                          n_iter_no_change=None, presort='deprecated',\n",
            "                          random_state=None, subsample=1.0, tol=0.0001,\n",
            "                          validation_fraction=0.1, verbose=0, warm_start=False)\n",
            "R2: 0.9170328741202223\n",
            "RMSE: 3.1142842654619143\n",
            "Test\n",
            "R2: 0.9170328741202223\n",
            "RMSE: 3.1142842654619143\n",
            "Accuracy: 0.50 (+/- 0.09)\n",
            "Ventricles_Norm\n",
            "GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse',\n",
            "                          init=None, learning_rate=0.05, loss='huber',\n",
            "                          max_depth=3, max_features='sqrt', max_leaf_nodes=None,\n",
            "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                          min_samples_leaf=15, min_samples_split=10,\n",
            "                          min_weight_fraction_leaf=0.0, n_estimators=3000,\n",
            "                          n_iter_no_change=None, presort='deprecated',\n",
            "                          random_state=None, subsample=1.0, tol=0.0001,\n",
            "                          validation_fraction=0.1, verbose=0, warm_start=False)\n",
            "R2: 0.9946344880811956\n",
            "RMSE: 0.0008681253204233942\n",
            "Test\n",
            "R2: 0.9946344880811956\n",
            "RMSE: 0.0008681253204233942\n",
            "Accuracy: 0.50 (+/- 0.03)\n",
            "MMSE\n",
            "GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse',\n",
            "                          init=None, learning_rate=0.05, loss='huber',\n",
            "                          max_depth=3, max_features='sqrt', max_leaf_nodes=None,\n",
            "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                          min_samples_leaf=15, min_samples_split=10,\n",
            "                          min_weight_fraction_leaf=0.0, n_estimators=3000,\n",
            "                          n_iter_no_change=None, presort='deprecated',\n",
            "                          random_state=None, subsample=1.0, tol=0.0001,\n",
            "                          validation_fraction=0.1, verbose=0, warm_start=False)\n",
            "R2: 0.8574598969106111\n",
            "RMSE: 1.3155858316963318\n",
            "Test\n",
            "R2: 0.8574598969106111\n",
            "RMSE: 1.3155858316963318\n",
            "Accuracy: 0.36 (+/- 0.24)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nm11Adl14fWB",
        "colab_type": "text"
      },
      "source": [
        "###Neural Network for regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgB3ndxC5gf7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#### Neural networks\n",
        "\n",
        "\n",
        "def random_batch(x_train, y_train, batch_size):\n",
        "    \n",
        "    num = x_train.shape[0]\n",
        "\n",
        "    # Create a random index.\n",
        "    idx = np.random.choice(num,\n",
        "                           size=batch_size,\n",
        "                           replace=False)\n",
        "\n",
        "    # Use the random index to select random images and labels.\n",
        "    x_batch = x_train[idx]\n",
        "    y_batch = y_train[idx]\n",
        "\n",
        "    return x_batch, y_batch"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDlWOdsD5ggB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def NN(x_train,y_train,x_val,y_val):\n",
        "    # Convert y_train shape to ?x1\n",
        "    y_train = y_train.values.reshape(-1,1)\n",
        "    y_val = y_val.values.reshape(-1,1)\n",
        "\n",
        "    n_input = x_train.shape[1]\n",
        "    n_hidden1 = 128\n",
        "    n_hidden2 = 512\n",
        "    n_hidden3 = 1024\n",
        "    n_output = 1\n",
        "    learning_rate = 0.001\n",
        "    epochs = 200\n",
        "    batch_size = 25\n",
        "    REGULARIZATION_RATE = 0.0001\n",
        "\n",
        "    X = tf.compat.v1.placeholder(tf.float32,[None,n_input])\n",
        "    y_gt = tf.compat.v1.placeholder(tf.float32,[None,n_output])\n",
        "\n",
        "\n",
        "    initializer = tf.contrib.layers.variance_scaling_initializer(factor=2.0, mode='FAN_IN', uniform=False)\n",
        "    W1 = tf.Variable(initializer([n_input,n_hidden1]))\n",
        "    b1 = tf.Variable(tf.constant(0.1,shape=[n_hidden1]))\n",
        "    H1 = tf.nn.relu(tf.matmul(X,W1)+b1)\n",
        "\n",
        "    W2 = tf.Variable(initializer([n_hidden1,n_hidden2]))\n",
        "    b2 = tf.Variable(tf.constant(0.1,shape=[n_hidden2]))\n",
        "    H2 = tf.nn.relu(tf.matmul(H1,W2)+b2)\n",
        "\n",
        "    W3 = tf.Variable(initializer([n_hidden2,n_hidden3]))\n",
        "    b3 = tf.Variable(tf.constant(0.1,shape=[n_hidden3]))\n",
        "    H3 = tf.nn.relu(tf.matmul(H2,W3)+b3)\n",
        "\n",
        "    W_out = tf.Variable(initializer([n_hidden3,n_output]))\n",
        "    b_out = tf.Variable(tf.constant(0.1,shape=[n_output]))\n",
        "    y_pred = tf.matmul(H3,W_out)+b_out\n",
        "\n",
        "    tr_losses=[]\n",
        "    te_losses=[]\n",
        "    loss = tf.reduce_mean(tf.losses.mean_squared_error(labels=y_gt,predictions=y_pred)) \n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "    train_step = optimizer.minimize(loss)\n",
        "\n",
        "    sess = tf.InteractiveSession()\n",
        "    tf.global_variables_initializer().run()\n",
        "\n",
        "    for iter in range(epochs):\n",
        "\n",
        "        sess.run(train_step, feed_dict={X:x_train, y_gt:y_train})\n",
        "        if iter%1 == 0:\n",
        "            train_loss = sess.run(loss, feed_dict={X:x_train, y_gt:y_train})\n",
        "            validation_loss = sess.run(loss, feed_dict={X:x_val, y_gt:y_val})\n",
        "            print(\"Iter %d, training loss %f, validation loss %f\" % (iter, train_loss, validation_loss))\n",
        "            tr_losses.append(train_loss)\n",
        "            te_losses.append(validation_loss)\n",
        "    train_pred = sess.run(y_pred,feed_dict={X:x_train})\n",
        "    val_pred =sess.run(y_pred, feed_dict={X:x_val})\n",
        "    print(\"Size of the model predictions for training is: \", train_pred.shape)\n",
        "    print(\"Size of the model predictions for validation is: \", val_pred.shape)\n",
        "    return train_pred, val_pred, tr_losses, te_losses"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vonMd4mL5ggM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_NN(epochs, tr_losses,te_losses):\n",
        "    import matplotlib.pyplot as plt\n",
        "    import math\n",
        "    epoch=list(range(1,epochs+1))\n",
        "    plt.figure()\n",
        "    tr_loss=[math.sqrt(x) for x in tr_losses]\n",
        "    te_loss=[math.sqrt(x) for x in te_losses]\n",
        "    \n",
        "    plt.plot(epoch,tr_losses, label='training loss')\n",
        "    plt.plot(epoch,te_losses,label='validation loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    \n",
        "    return tr_loss, te_loss"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AG-jMllscN3e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "157730df-3b55-4891-8af5-501169613713"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrQNCXOU5ggS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2a9f7153-24c0-4e47-c481-34891d37a6db"
      },
      "source": [
        "epochs=200\n",
        "print(\"ADAS13\")\n",
        "adas_train_pred, adas_val_pred, tr_losses, te_losses=NN(x_train,y_train_adas13,x_val,y_val_adas13)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ADAS13\n",
            "Iter 0, training loss 293.817322, validation loss 257.581055\n",
            "Iter 1, training loss 230.372101, validation loss 205.371033\n",
            "Iter 2, training loss 140.966278, validation loss 140.904816\n",
            "Iter 3, training loss 148.564850, validation loss 169.345581\n",
            "Iter 4, training loss 165.376846, validation loss 195.805649\n",
            "Iter 5, training loss 139.310654, validation loss 168.944901\n",
            "Iter 6, training loss 109.964821, validation loss 132.728699\n",
            "Iter 7, training loss 124.560120, validation loss 139.680145\n",
            "Iter 8, training loss 128.017563, validation loss 145.458374\n",
            "Iter 9, training loss 104.942329, validation loss 131.319138\n",
            "Iter 10, training loss 91.143593, validation loss 128.233871\n",
            "Iter 11, training loss 93.180046, validation loss 139.248230\n",
            "Iter 12, training loss 93.741348, validation loss 145.283707\n",
            "Iter 13, training loss 84.053024, validation loss 137.366959\n",
            "Iter 14, training loss 71.465286, validation loss 123.775780\n",
            "Iter 15, training loss 66.601761, validation loss 116.710899\n",
            "Iter 16, training loss 68.658127, validation loss 117.565681\n",
            "Iter 17, training loss 66.132568, validation loss 116.929726\n",
            "Iter 18, training loss 56.622375, validation loss 112.624237\n",
            "Iter 19, training loss 49.287376, validation loss 112.243912\n",
            "Iter 20, training loss 48.291290, validation loss 117.761726\n",
            "Iter 21, training loss 48.017502, validation loss 121.720009\n",
            "Iter 22, training loss 43.540993, validation loss 118.231667\n",
            "Iter 23, training loss 37.713943, validation loss 110.340591\n",
            "Iter 24, training loss 36.156246, validation loss 105.181740\n",
            "Iter 25, training loss 37.409485, validation loss 104.065094\n",
            "Iter 26, training loss 35.012360, validation loss 102.650467\n",
            "Iter 27, training loss 30.707987, validation loss 102.089989\n",
            "Iter 28, training loss 29.652199, validation loss 105.233337\n",
            "Iter 29, training loss 30.161205, validation loss 108.038445\n",
            "Iter 30, training loss 28.249874, validation loss 105.421509\n",
            "Iter 31, training loss 25.205763, validation loss 99.361053\n",
            "Iter 32, training loss 24.387463, validation loss 95.074814\n",
            "Iter 33, training loss 24.745430, validation loss 93.578117\n",
            "Iter 34, training loss 23.288816, validation loss 92.869560\n",
            "Iter 35, training loss 21.306435, validation loss 93.545212\n",
            "Iter 36, training loss 20.974804, validation loss 96.140656\n",
            "Iter 37, training loss 20.916180, validation loss 97.667000\n",
            "Iter 38, training loss 19.618586, validation loss 96.096779\n",
            "Iter 39, training loss 18.296019, validation loss 93.306747\n",
            "Iter 40, training loss 18.114466, validation loss 91.789444\n",
            "Iter 41, training loss 17.882244, validation loss 91.586082\n",
            "Iter 42, training loss 16.787798, validation loss 92.141914\n",
            "Iter 43, training loss 16.009550, validation loss 93.815956\n",
            "Iter 44, training loss 15.926225, validation loss 95.666794\n",
            "Iter 45, training loss 15.504604, validation loss 95.767014\n",
            "Iter 46, training loss 14.648088, validation loss 94.095207\n",
            "Iter 47, training loss 14.226942, validation loss 92.371727\n",
            "Iter 48, training loss 14.136700, validation loss 91.573242\n",
            "Iter 49, training loss 13.631528, validation loss 91.550247\n",
            "Iter 50, training loss 13.064708, validation loss 92.307411\n",
            "Iter 51, training loss 12.924086, validation loss 93.398392\n",
            "Iter 52, training loss 12.727759, validation loss 93.553215\n",
            "Iter 53, training loss 12.235852, validation loss 92.445190\n",
            "Iter 54, training loss 11.926872, validation loss 91.151375\n",
            "Iter 55, training loss 11.813809, validation loss 90.532196\n",
            "Iter 56, training loss 11.470635, validation loss 90.614296\n",
            "Iter 57, training loss 11.107633, validation loss 91.317642\n",
            "Iter 58, training loss 10.977818, validation loss 92.139145\n",
            "Iter 59, training loss 10.769978, validation loss 92.171356\n",
            "Iter 60, training loss 10.444489, validation loss 91.401039\n",
            "Iter 61, training loss 10.278308, validation loss 90.674622\n",
            "Iter 62, training loss 10.126357, validation loss 90.475189\n",
            "Iter 63, training loss 9.842834, validation loss 90.816978\n",
            "Iter 64, training loss 9.659732, validation loss 91.489609\n",
            "Iter 65, training loss 9.532741, validation loss 91.836266\n",
            "Iter 66, training loss 9.303709, validation loss 91.460739\n",
            "Iter 67, training loss 9.126833, validation loss 90.863548\n",
            "Iter 68, training loss 9.010077, validation loss 90.640907\n",
            "Iter 69, training loss 8.813920, validation loss 90.929474\n",
            "Iter 70, training loss 8.651532, validation loss 91.566238\n",
            "Iter 71, training loss 8.539429, validation loss 92.014145\n",
            "Iter 72, training loss 8.370232, validation loss 91.897110\n",
            "Iter 73, training loss 8.226006, validation loss 91.560028\n",
            "Iter 74, training loss 8.116541, validation loss 91.489891\n",
            "Iter 75, training loss 7.962329, validation loss 91.816605\n",
            "Iter 76, training loss 7.833530, validation loss 92.359459\n",
            "Iter 77, training loss 7.727507, validation loss 92.667488\n",
            "Iter 78, training loss 7.588645, validation loss 92.535576\n",
            "Iter 79, training loss 7.480236, validation loss 92.305801\n",
            "Iter 80, training loss 7.375103, validation loss 92.335670\n",
            "Iter 81, training loss 7.248000, validation loss 92.671333\n",
            "Iter 82, training loss 7.150570, validation loss 93.041878\n",
            "Iter 83, training loss 7.043622, validation loss 93.086143\n",
            "Iter 84, training loss 6.935253, validation loss 92.872307\n",
            "Iter 85, training loss 6.846569, validation loss 92.803497\n",
            "Iter 86, training loss 6.740542, validation loss 93.044876\n",
            "Iter 87, training loss 6.650585, validation loss 93.398232\n",
            "Iter 88, training loss 6.559197, validation loss 93.528885\n",
            "Iter 89, training loss 6.463107, validation loss 93.441170\n",
            "Iter 90, training loss 6.381047, validation loss 93.439804\n",
            "Iter 91, training loss 6.289931, validation loss 93.681465\n",
            "Iter 92, training loss 6.208424, validation loss 94.014534\n",
            "Iter 93, training loss 6.126448, validation loss 94.159042\n",
            "Iter 94, training loss 6.042122, validation loss 94.126663\n",
            "Iter 95, training loss 5.965536, validation loss 94.186768\n",
            "Iter 96, training loss 5.883445, validation loss 94.459396\n",
            "Iter 97, training loss 5.810700, validation loss 94.763611\n",
            "Iter 98, training loss 5.735240, validation loss 94.866066\n",
            "Iter 99, training loss 5.663825, validation loss 94.867149\n",
            "Iter 100, training loss 5.591948, validation loss 94.976936\n",
            "Iter 101, training loss 5.521762, validation loss 95.183632\n",
            "Iter 102, training loss 5.454567, validation loss 95.307861\n",
            "Iter 103, training loss 5.386569, validation loss 95.318359\n",
            "Iter 104, training loss 5.321993, validation loss 95.397934\n",
            "Iter 105, training loss 5.256169, validation loss 95.605354\n",
            "Iter 106, training loss 5.194320, validation loss 95.787552\n",
            "Iter 107, training loss 5.132657, validation loss 95.825951\n",
            "Iter 108, training loss 5.073289, validation loss 95.866982\n",
            "Iter 109, training loss 5.012788, validation loss 96.012733\n",
            "Iter 110, training loss 4.955192, validation loss 96.104019\n",
            "Iter 111, training loss 4.898249, validation loss 96.092224\n",
            "Iter 112, training loss 4.842688, validation loss 96.141586\n",
            "Iter 113, training loss 4.787887, validation loss 96.297897\n",
            "Iter 114, training loss 4.734255, validation loss 96.379730\n",
            "Iter 115, training loss 4.682036, validation loss 96.382538\n",
            "Iter 116, training loss 4.630692, validation loss 96.440399\n",
            "Iter 117, training loss 4.580807, validation loss 96.537041\n",
            "Iter 118, training loss 4.531973, validation loss 96.549347\n",
            "Iter 119, training loss 4.484898, validation loss 96.567154\n",
            "Iter 120, training loss 4.437117, validation loss 96.686768\n",
            "Iter 121, training loss 4.390905, validation loss 96.802307\n",
            "Iter 122, training loss 4.344771, validation loss 96.820145\n",
            "Iter 123, training loss 4.300317, validation loss 96.862160\n",
            "Iter 124, training loss 4.256114, validation loss 96.960541\n",
            "Iter 125, training loss 4.212529, validation loss 96.988312\n",
            "Iter 126, training loss 4.171495, validation loss 97.018005\n",
            "Iter 127, training loss 4.128963, validation loss 97.121094\n",
            "Iter 128, training loss 4.088173, validation loss 97.161957\n",
            "Iter 129, training loss 4.048403, validation loss 97.168533\n",
            "Iter 130, training loss 4.008787, validation loss 97.249847\n",
            "Iter 131, training loss 3.970029, validation loss 97.288284\n",
            "Iter 132, training loss 3.932132, validation loss 97.288208\n",
            "Iter 133, training loss 3.894161, validation loss 97.336639\n",
            "Iter 134, training loss 3.857627, validation loss 97.375671\n",
            "Iter 135, training loss 3.821539, validation loss 97.434547\n",
            "Iter 136, training loss 3.784640, validation loss 97.481461\n",
            "Iter 137, training loss 3.750866, validation loss 97.503349\n",
            "Iter 138, training loss 3.716028, validation loss 97.514511\n",
            "Iter 139, training loss 3.682412, validation loss 97.586182\n",
            "Iter 140, training loss 3.648925, validation loss 97.603371\n",
            "Iter 141, training loss 3.615931, validation loss 97.628929\n",
            "Iter 142, training loss 3.583649, validation loss 97.696007\n",
            "Iter 143, training loss 3.551528, validation loss 97.696091\n",
            "Iter 144, training loss 3.520333, validation loss 97.759712\n",
            "Iter 145, training loss 3.489131, validation loss 97.797585\n",
            "Iter 146, training loss 3.458666, validation loss 97.851166\n",
            "Iter 147, training loss 3.428799, validation loss 97.867538\n",
            "Iter 148, training loss 3.398923, validation loss 97.883583\n",
            "Iter 149, training loss 3.370359, validation loss 97.970207\n",
            "Iter 150, training loss 3.341472, validation loss 98.001884\n",
            "Iter 151, training loss 3.313520, validation loss 98.050232\n",
            "Iter 152, training loss 3.286068, validation loss 98.009109\n",
            "Iter 153, training loss 3.260063, validation loss 98.151253\n",
            "Iter 154, training loss 3.234848, validation loss 98.078186\n",
            "Iter 155, training loss 3.209960, validation loss 98.252251\n",
            "Iter 156, training loss 3.183965, validation loss 98.073547\n",
            "Iter 157, training loss 3.158139, validation loss 98.302803\n",
            "Iter 158, training loss 3.131178, validation loss 98.186119\n",
            "Iter 159, training loss 3.106252, validation loss 98.292145\n",
            "Iter 160, training loss 3.080721, validation loss 98.288261\n",
            "Iter 161, training loss 3.057566, validation loss 98.291298\n",
            "Iter 162, training loss 3.035331, validation loss 98.387878\n",
            "Iter 163, training loss 3.012436, validation loss 98.323250\n",
            "Iter 164, training loss 2.989311, validation loss 98.501274\n",
            "Iter 165, training loss 2.967629, validation loss 98.403931\n",
            "Iter 166, training loss 2.945483, validation loss 98.581161\n",
            "Iter 167, training loss 2.923752, validation loss 98.419769\n",
            "Iter 168, training loss 2.903861, validation loss 98.640350\n",
            "Iter 169, training loss 2.883296, validation loss 98.492996\n",
            "Iter 170, training loss 2.863534, validation loss 98.767914\n",
            "Iter 171, training loss 2.843877, validation loss 98.556595\n",
            "Iter 172, training loss 2.821499, validation loss 98.820152\n",
            "Iter 173, training loss 2.800904, validation loss 98.658424\n",
            "Iter 174, training loss 2.779919, validation loss 98.868591\n",
            "Iter 175, training loss 2.759671, validation loss 98.765999\n",
            "Iter 176, training loss 2.739989, validation loss 98.877342\n",
            "Iter 177, training loss 2.721391, validation loss 98.848061\n",
            "Iter 178, training loss 2.702032, validation loss 98.941315\n",
            "Iter 179, training loss 2.684113, validation loss 98.952347\n",
            "Iter 180, training loss 2.665981, validation loss 99.009598\n",
            "Iter 181, training loss 2.647673, validation loss 99.031052\n",
            "Iter 182, training loss 2.631229, validation loss 99.116592\n",
            "Iter 183, training loss 2.613039, validation loss 99.107689\n",
            "Iter 184, training loss 2.597071, validation loss 99.222717\n",
            "Iter 185, training loss 2.582284, validation loss 99.151192\n",
            "Iter 186, training loss 2.573009, validation loss 99.479767\n",
            "Iter 187, training loss 2.576972, validation loss 99.144226\n",
            "Iter 188, training loss 2.598679, validation loss 99.864899\n",
            "Iter 189, training loss 2.615731, validation loss 99.023430\n",
            "Iter 190, training loss 2.573712, validation loss 99.968742\n",
            "Iter 191, training loss 2.497335, validation loss 99.363159\n",
            "Iter 192, training loss 2.479701, validation loss 99.457756\n",
            "Iter 193, training loss 2.511888, validation loss 100.088547\n",
            "Iter 194, training loss 2.509755, validation loss 99.382126\n",
            "Iter 195, training loss 2.455315, validation loss 100.007500\n",
            "Iter 196, training loss 2.415280, validation loss 99.714813\n",
            "Iter 197, training loss 2.427125, validation loss 99.483826\n",
            "Iter 198, training loss 2.435938, validation loss 100.130325\n",
            "Iter 199, training loss 2.400319, validation loss 99.612152\n",
            "Size of the model predictions for training is:  (6716, 1)\n",
            "Size of the model predictions for validation is:  (2238, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQHtV2g35ggW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "outputId": "fe8f0bac-f976-4a20-9936-f182caa45543"
      },
      "source": [
        "tr_loss, te_loss=plot_NN(epochs, tr_losses, te_losses)\n",
        "print(\"Best epoch number is: \", te_loss.index(min(te_loss)))\n",
        "print(\"The lowest validation loss is: \", min(te_loss))\n",
        "print(\"At the same epoch, the training loss is: \", tr_loss[te_loss.index(min(te_loss))])"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Best epoch number is:  62\n",
            "The lowest validation loss is:  9.511844679607862\n",
            "At the same epoch, the training loss is:  3.1821937525160604\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gidNOs1H5gge",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e97f4540-be58-4de1-82b4-10d0bb518f16"
      },
      "source": [
        "print(\"Ventricles_Norm\")\n",
        "ventricles_train_pred, ventricles_val_pred, tr_losses, te_losses=ventricles_pred=NN(x_train,y_train_ventricles,x_val,y_val_ventricles)\n"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ventricles_Norm\n",
            "Iter 0, training loss 255.889114, validation loss 252.257202\n",
            "Iter 1, training loss 21.185781, validation loss 20.388475\n",
            "Iter 2, training loss 41.053009, validation loss 41.995728\n",
            "Iter 3, training loss 109.962128, validation loss 111.742432\n",
            "Iter 4, training loss 82.858818, validation loss 84.567451\n",
            "Iter 5, training loss 28.870678, validation loss 29.870518\n",
            "Iter 6, training loss 1.667230, validation loss 1.932180\n",
            "Iter 7, training loss 7.307764, validation loss 7.250786\n",
            "Iter 8, training loss 25.843697, validation loss 25.845741\n",
            "Iter 9, training loss 36.873428, validation loss 37.030052\n",
            "Iter 10, training loss 33.628742, validation loss 33.858173\n",
            "Iter 11, training loss 21.230574, validation loss 21.404318\n",
            "Iter 12, training loss 8.386953, validation loss 8.457646\n",
            "Iter 13, training loss 1.200825, validation loss 1.251557\n",
            "Iter 14, training loss 1.081386, validation loss 1.251626\n",
            "Iter 15, training loss 5.596549, validation loss 5.991350\n",
            "Iter 16, training loss 10.778072, validation loss 11.405593\n",
            "Iter 17, training loss 13.467667, validation loss 14.241806\n",
            "Iter 18, training loss 12.604354, validation loss 13.388914\n",
            "Iter 19, training loss 9.104211, validation loss 9.770723\n",
            "Iter 20, training loss 4.851113, validation loss 5.325153\n",
            "Iter 21, training loss 1.589450, validation loss 1.862691\n",
            "Iter 22, training loss 0.232450, validation loss 0.344629\n",
            "Iter 23, training loss 0.713496, validation loss 0.723709\n",
            "Iter 24, training loss 2.263172, validation loss 2.226151\n",
            "Iter 25, training loss 3.881471, validation loss 3.831788\n",
            "Iter 26, training loss 4.782103, validation loss 4.732851\n",
            "Iter 27, training loss 4.647829, validation loss 4.599220\n",
            "Iter 28, training loss 3.644178, validation loss 3.596578\n",
            "Iter 29, training loss 2.246904, validation loss 2.209311\n",
            "Iter 30, training loss 0.991334, validation loss 0.983953\n",
            "Iter 31, training loss 0.257538, validation loss 0.305971\n",
            "Iter 32, training loss 0.159879, validation loss 0.285846\n",
            "Iter 33, training loss 0.556553, validation loss 0.769245\n",
            "Iter 34, training loss 1.151775, validation loss 1.442991\n",
            "Iter 35, training loss 1.635664, validation loss 1.980692\n",
            "Iter 36, training loss 1.803735, validation loss 2.167602\n",
            "Iter 37, training loss 1.614657, validation loss 1.961233\n",
            "Iter 38, training loss 1.175023, validation loss 1.475363\n",
            "Iter 39, training loss 0.670406, validation loss 0.907898\n",
            "Iter 40, training loss 0.279211, validation loss 0.450617\n",
            "Iter 41, training loss 0.105868, validation loss 0.218567\n",
            "Iter 42, training loss 0.154545, validation loss 0.221999\n",
            "Iter 43, training loss 0.345656, validation loss 0.382725\n",
            "Iter 44, training loss 0.561146, validation loss 0.580900\n",
            "Iter 45, training loss 0.696526, validation loss 0.709099\n",
            "Iter 46, training loss 0.698621, validation loss 0.711832\n",
            "Iter 47, training loss 0.577111, validation loss 0.597803\n",
            "Iter 48, training loss 0.389696, validation loss 0.424769\n",
            "Iter 49, training loss 0.211072, validation loss 0.267476\n",
            "Iter 50, training loss 0.100457, validation loss 0.184144\n",
            "Iter 51, training loss 0.080639, validation loss 0.194908\n",
            "Iter 52, training loss 0.134945, validation loss 0.279040\n",
            "Iter 53, training loss 0.220444, validation loss 0.389106\n",
            "Iter 54, training loss 0.289649, validation loss 0.473819\n",
            "Iter 55, training loss 0.310583, validation loss 0.499197\n",
            "Iter 56, training loss 0.277291, validation loss 0.459511\n",
            "Iter 57, training loss 0.207892, validation loss 0.375124\n",
            "Iter 58, training loss 0.132938, validation loss 0.280061\n",
            "Iter 59, training loss 0.080475, validation loss 0.206087\n",
            "Iter 60, training loss 0.064605, validation loss 0.170471\n",
            "Iter 61, training loss 0.081848, validation loss 0.171816\n",
            "Iter 62, training loss 0.115515, validation loss 0.194400\n",
            "Iter 63, training loss 0.145026, validation loss 0.217737\n",
            "Iter 64, training loss 0.155421, validation loss 0.226548\n",
            "Iter 65, training loss 0.142895, validation loss 0.216588\n",
            "Iter 66, training loss 0.114563, validation loss 0.194481\n",
            "Iter 67, training loss 0.083479, validation loss 0.172627\n",
            "Iter 68, training loss 0.061859, validation loss 0.162264\n",
            "Iter 69, training loss 0.055774, validation loss 0.168141\n",
            "Iter 70, training loss 0.063433, validation loss 0.186923\n",
            "Iter 71, training loss 0.077284, validation loss 0.209558\n",
            "Iter 72, training loss 0.088383, validation loss 0.225974\n",
            "Iter 73, training loss 0.090707, validation loss 0.229638\n",
            "Iter 74, training loss 0.083429, validation loss 0.219921\n",
            "Iter 75, training loss 0.070420, validation loss 0.201513\n",
            "Iter 76, training loss 0.057653, validation loss 0.181581\n",
            "Iter 77, training loss 0.050043, validation loss 0.166312\n",
            "Iter 78, training loss 0.049296, validation loss 0.158546\n",
            "Iter 79, training loss 0.053637, validation loss 0.157328\n",
            "Iter 80, training loss 0.059234, validation loss 0.159295\n",
            "Iter 81, training loss 0.062373, validation loss 0.160905\n",
            "Iter 82, training loss 0.061226, validation loss 0.160267\n",
            "Iter 83, training loss 0.056419, validation loss 0.157763\n",
            "Iter 84, training loss 0.050307, validation loss 0.155354\n",
            "Iter 85, training loss 0.045527, validation loss 0.155156\n",
            "Iter 86, training loss 0.043641, validation loss 0.158090\n",
            "Iter 87, training loss 0.044529, validation loss 0.163379\n",
            "Iter 88, training loss 0.046731, validation loss 0.168960\n",
            "Iter 89, training loss 0.048397, validation loss 0.172565\n",
            "Iter 90, training loss 0.048290, validation loss 0.172807\n",
            "Iter 91, training loss 0.046322, validation loss 0.169702\n",
            "Iter 92, training loss 0.043410, validation loss 0.164497\n",
            "Iter 93, training loss 0.040830, validation loss 0.158923\n",
            "Iter 94, training loss 0.039472, validation loss 0.154370\n",
            "Iter 95, training loss 0.039441, validation loss 0.151414\n",
            "Iter 96, training loss 0.040133, validation loss 0.149817\n",
            "Iter 97, training loss 0.040672, validation loss 0.148897\n",
            "Iter 98, training loss 0.040405, validation loss 0.148111\n",
            "Iter 99, training loss 0.039272, validation loss 0.147395\n",
            "Iter 100, training loss 0.037849, validation loss 0.147221\n",
            "Iter 101, training loss 0.036948, validation loss 0.148025\n",
            "Iter 102, training loss 0.036795, validation loss 0.149700\n",
            "Iter 103, training loss 0.036347, validation loss 0.150059\n",
            "Iter 104, training loss 0.035642, validation loss 0.150133\n",
            "Iter 105, training loss 0.034927, validation loss 0.150020\n",
            "Iter 106, training loss 0.034415, validation loss 0.149807\n",
            "Iter 107, training loss 0.033982, validation loss 0.149392\n",
            "Iter 108, training loss 0.033587, validation loss 0.148811\n",
            "Iter 109, training loss 0.033227, validation loss 0.148139\n",
            "Iter 110, training loss 0.032903, validation loss 0.147478\n",
            "Iter 111, training loss 0.032600, validation loss 0.146877\n",
            "Iter 112, training loss 0.032297, validation loss 0.146384\n",
            "Iter 113, training loss 0.031979, validation loss 0.146005\n",
            "Iter 114, training loss 0.031640, validation loss 0.145737\n",
            "Iter 115, training loss 0.031291, validation loss 0.145564\n",
            "Iter 116, training loss 0.030947, validation loss 0.145461\n",
            "Iter 117, training loss 0.030618, validation loss 0.145385\n",
            "Iter 118, training loss 0.030307, validation loss 0.145285\n",
            "Iter 119, training loss 0.030005, validation loss 0.145102\n",
            "Iter 120, training loss 0.029699, validation loss 0.144789\n",
            "Iter 121, training loss 0.029383, validation loss 0.144322\n",
            "Iter 122, training loss 0.029060, validation loss 0.143732\n",
            "Iter 123, training loss 0.028739, validation loss 0.143079\n",
            "Iter 124, training loss 0.028428, validation loss 0.142403\n",
            "Iter 125, training loss 0.028132, validation loss 0.141759\n",
            "Iter 126, training loss 0.027846, validation loss 0.141198\n",
            "Iter 127, training loss 0.027563, validation loss 0.140754\n",
            "Iter 128, training loss 0.027273, validation loss 0.140434\n",
            "Iter 129, training loss 0.026981, validation loss 0.140218\n",
            "Iter 130, training loss 0.026692, validation loss 0.140077\n",
            "Iter 131, training loss 0.026414, validation loss 0.139963\n",
            "Iter 132, training loss 0.026148, validation loss 0.139828\n",
            "Iter 133, training loss 0.025888, validation loss 0.139613\n",
            "Iter 134, training loss 0.025630, validation loss 0.139297\n",
            "Iter 135, training loss 0.025370, validation loss 0.138877\n",
            "Iter 136, training loss 0.025109, validation loss 0.138395\n",
            "Iter 137, training loss 0.024852, validation loss 0.137888\n",
            "Iter 138, training loss 0.024602, validation loss 0.137389\n",
            "Iter 139, training loss 0.024359, validation loss 0.136942\n",
            "Iter 140, training loss 0.024116, validation loss 0.136573\n",
            "Iter 141, training loss 0.023871, validation loss 0.136289\n",
            "Iter 142, training loss 0.023630, validation loss 0.136059\n",
            "Iter 143, training loss 0.023397, validation loss 0.135869\n",
            "Iter 144, training loss 0.023166, validation loss 0.135693\n",
            "Iter 145, training loss 0.022941, validation loss 0.135498\n",
            "Iter 146, training loss 0.022719, validation loss 0.135239\n",
            "Iter 147, training loss 0.022497, validation loss 0.134910\n",
            "Iter 148, training loss 0.022276, validation loss 0.134523\n",
            "Iter 149, training loss 0.022060, validation loss 0.134120\n",
            "Iter 150, training loss 0.021847, validation loss 0.133736\n",
            "Iter 151, training loss 0.021637, validation loss 0.133392\n",
            "Iter 152, training loss 0.021430, validation loss 0.133097\n",
            "Iter 153, training loss 0.021224, validation loss 0.132835\n",
            "Iter 154, training loss 0.021020, validation loss 0.132585\n",
            "Iter 155, training loss 0.020819, validation loss 0.132320\n",
            "Iter 156, training loss 0.020620, validation loss 0.132042\n",
            "Iter 157, training loss 0.020423, validation loss 0.131745\n",
            "Iter 158, training loss 0.020226, validation loss 0.131420\n",
            "Iter 159, training loss 0.020032, validation loss 0.131061\n",
            "Iter 160, training loss 0.019841, validation loss 0.130683\n",
            "Iter 161, training loss 0.019651, validation loss 0.130319\n",
            "Iter 162, training loss 0.019464, validation loss 0.129991\n",
            "Iter 163, training loss 0.019279, validation loss 0.129708\n",
            "Iter 164, training loss 0.019096, validation loss 0.129470\n",
            "Iter 165, training loss 0.018915, validation loss 0.129244\n",
            "Iter 166, training loss 0.018737, validation loss 0.129021\n",
            "Iter 167, training loss 0.018559, validation loss 0.128770\n",
            "Iter 168, training loss 0.018382, validation loss 0.128480\n",
            "Iter 169, training loss 0.018207, validation loss 0.128191\n",
            "Iter 170, training loss 0.018033, validation loss 0.127883\n",
            "Iter 171, training loss 0.017862, validation loss 0.127589\n",
            "Iter 172, training loss 0.017692, validation loss 0.127328\n",
            "Iter 173, training loss 0.017524, validation loss 0.127067\n",
            "Iter 174, training loss 0.017357, validation loss 0.126799\n",
            "Iter 175, training loss 0.017192, validation loss 0.126518\n",
            "Iter 176, training loss 0.017028, validation loss 0.126253\n",
            "Iter 177, training loss 0.016866, validation loss 0.125952\n",
            "Iter 178, training loss 0.016705, validation loss 0.125618\n",
            "Iter 179, training loss 0.016546, validation loss 0.125339\n",
            "Iter 180, training loss 0.016388, validation loss 0.125025\n",
            "Iter 181, training loss 0.016232, validation loss 0.124783\n",
            "Iter 182, training loss 0.016076, validation loss 0.124471\n",
            "Iter 183, training loss 0.015923, validation loss 0.124164\n",
            "Iter 184, training loss 0.015770, validation loss 0.123889\n",
            "Iter 185, training loss 0.015619, validation loss 0.123592\n",
            "Iter 186, training loss 0.015470, validation loss 0.123303\n",
            "Iter 187, training loss 0.015322, validation loss 0.123056\n",
            "Iter 188, training loss 0.015176, validation loss 0.122774\n",
            "Iter 189, training loss 0.015032, validation loss 0.122562\n",
            "Iter 190, training loss 0.014886, validation loss 0.122214\n",
            "Iter 191, training loss 0.014743, validation loss 0.121975\n",
            "Iter 192, training loss 0.014604, validation loss 0.121609\n",
            "Iter 193, training loss 0.014470, validation loss 0.121421\n",
            "Iter 194, training loss 0.014324, validation loss 0.120991\n",
            "Iter 195, training loss 0.014193, validation loss 0.120629\n",
            "Iter 196, training loss 0.014061, validation loss 0.120539\n",
            "Iter 197, training loss 0.013920, validation loss 0.120200\n",
            "Iter 198, training loss 0.013801, validation loss 0.119723\n",
            "Iter 199, training loss 0.013657, validation loss 0.119715\n",
            "Size of the model predictions for training is:  (6716, 1)\n",
            "Size of the model predictions for validation is:  (2238, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYzJr-0C5ggu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "outputId": "be3fad43-bca6-423e-e3d9-2eb2f36e753a"
      },
      "source": [
        "epochs=200\n",
        "tr_loss, te_loss=plot_NN(epochs, tr_losses, te_losses)\n",
        "print(\"Best epoch number is: \", te_loss.index(min(te_loss)))\n",
        "print(\"The lowest validation loss is: \", min(te_loss))\n",
        "print(\"At the same epoch, the training loss is: \", tr_loss[te_loss.index(min(te_loss))])"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEGCAYAAACKB4k+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZhU9Z3v8fe3qje6aaDZlE1Bg8q+iIpDDBqMQRy3JG6jcYmGxMfcxJuJN5hFTZ7rHTNjjGNizOhoYowbo5KYhLgGt5m4AAIiYgAF2ZdWmm7otep7/ziniuqVpqWqGs/n9Tz9dPWvzqn61unq+vTvd875HXN3REREAGL5LkBERLoPhYKIiKQpFEREJE2hICIiaQoFERFJK8h3AR9H//79ffjw4fkuQ0TkoLJo0aId7j6grfsO6lAYPnw4CxcuzHcZIiIHFTNb1959Gj4SEZE0hYKIiKQpFEREJO2g3qcgIrnX2NjIhg0bqKury3cpsg8lJSUMHTqUwsLCTq+jUBCR/bJhwwbKy8sZPnw4ZpbvcqQd7k5lZSUbNmxgxIgRnV5Pw0cisl/q6uro16+fAqGbMzP69eu33z06hYKI7DcFwsGhK7+nSIbCu1uque2Zd9lRU5/vUkREupVIhsLqbTXc8dfVfLi7Id+liMh+2rlzJ7/85S+7tO6sWbPYuXNnh8vccMMNPPfcc116/JaGDx/Ojh07Dshj5UokQyEW9qiSusCQyEGno1BoamrqcN358+fTp0+fDpf58Y9/zKmnntrl+g52WQsFMxtmZgvMbIWZvW1m3wrbbzKzjWa2JPyalbHO9Wa22szeNbPPZ6u2vjsWcW/hvxHbtTFbTyEiWTJnzhzWrFnDxIkTue6663jhhRc46aSTOOussxg9ejQA55xzDsceeyxjxozh7rvvTq+b+s997dq1jBo1iq9+9auMGTOG0047jdraWgAuv/xyHnvssfTyN954I5MnT2bcuHGsXLkSgO3bt/O5z32OMWPGcNVVV3H44Yfvs0dw2223MXbsWMaOHcvtt98OwO7duznjjDOYMGECY8eO5dFHH02/xtGjRzN+/Hi+853vHNgNuA/ZPCS1Cfhnd19sZuXAIjN7NrzvZ+5+a+bCZjYauBAYAwwGnjOzo9w9caALK6nfzgnxN1lVX32gH1okUn70x7dZsWnXAX3M0YN7ceOZY9q9/5ZbbmH58uUsWbIEgBdeeIHFixezfPny9KGX9913H3379qW2tpbjjjuOL37xi/Tr16/Z46xatYqHH36Ye+65h/PPP5/HH3+cSy65pNXz9e/fn8WLF/PLX/6SW2+9lf/8z//kRz/6EZ/97Ge5/vrreeqpp7j33ns7fE2LFi3i17/+Na+99hruzgknnMD06dN57733GDx4MH/+858BqKqqorKyknnz5rFy5UrMbJ/DXQda1noK7r7Z3ReHt6uBd4AhHaxyNvCIu9e7+/vAauD4bNRmsXiqxmw8vIjk2PHHH9/sWPw77riDCRMmMHXqVNavX8+qVatarTNixAgmTpwIwLHHHsvatWvbfOwvfOELrZZ55ZVXuPDCCwGYOXMmFRUVHdb3yiuvcO6551JWVkbPnj35whe+wMsvv8y4ceN49tln+e53v8vLL79M79696d27NyUlJVx55ZU88cQTlJaW7u/m+FhycvKamQ0HJgGvAdOAb5jZpcBCgt7ERwSB8WrGahtoI0TMbDYwG+Cwww7rWkHhTgVPdjz+KCId6+g/+lwqKytL337hhRd47rnn+Nvf/kZpaSknn3xym8fqFxcXp2/H4/H08FF7y8Xj8X3us9hfRx11FIsXL2b+/Pn84Ac/YMaMGdxwww28/vrrPP/88zz22GP84he/4K9//esBfd6OZH1Hs5n1BB4HrnX3XcBdwJHARGAz8NP9eTx3v9vdp7j7lAED2pwOvBM1BVmYTCa7tL6I5E95eTnV1e0P/VZVVVFRUUFpaSkrV67k1VdfbXfZrpo2bRpz584F4JlnnuGjjz7qcPmTTjqJ3//+9+zZs4fdu3czb948TjrpJDZt2kRpaSmXXHIJ1113HYsXL6ampoaqqipmzZrFz372M5YuXXrA6+9IVnsKZlZIEAgPuvsTAO6+NeP+e4A/hT9uBIZlrD40bDvwdcXCLDzwuytEJMv69evHtGnTGDt2LKeffjpnnHFGs/tnzpzJr371K0aNGsXRRx/N1KlTD3gNN954IxdddBEPPPAAJ554Ioceeijl5eXtLj958mQuv/xyjj8+GBG/6qqrmDRpEk8//TTXXXcdsViMwsJC7rrrLqqrqzn77LOpq6vD3bntttsOeP0dsWyNq1twKt39wIfufm1G+yB33xze/t/ACe5+oZmNAR4i2I8wGHgeGNnRjuYpU6Z4Vy6y8/YL/8WYF65ixRnzGH3cZ/d7fZEoe+eddxg1alS+y8ir+vp64vE4BQUF/O1vf+Pqq69O7/jubtr6fZnZInef0tby2ewpTAO+DLxlZqmt9T3gIjObCDiwFvgagLu/bWZzgRUERy5dk40jj2BvT8E1fCQiXfDBBx9w/vnnk0wmKSoq4p577sl3SQdM1kLB3V8B2pp4Y34H69wM3JytmtIsPPooqeEjEdl/I0eO5M0338x3GVkRyTOa0z0FV09BRCRTJEMhFlNPQUSkLZEMBdRTEBFpUyRDwUw7mkVE2hLNUEidp6DhI5FI6NmzJwCbNm3iS1/6UpvLnHzyyezrEPfbb7+dPXv2pH/uzFTcnXHTTTdx66237nvBHIhkKMTSRx+ppyASJYMHD07PgNoVLUOhM1NxH2wiGQqEO5qT2qcgctCZM2cOd955Z/rn1H/ZNTU1zJgxIz3N9R/+8IdW665du5axY8cCUFtby4UXXsioUaM499xzm819dPXVVzNlyhTGjBnDjTfeCAST7G3atIlTTjmFU045BWh+EZ22psbuaIru9ixZsoSpU6cyfvx4zj333PQUGnfccUd6Ou3UZHwvvvgiEydOZOLEiUyaNKnD6T86KycT4nU3sbiGj0QOiL/MgS1vHdjHPHQcnH5Lu3dfcMEFXHvttVxzzTUAzJ07l6effpqSkhLmzZtHr1692LFjB1OnTuWss85q9zrFd911F6WlpbzzzjssW7aMyZMnp++7+eab6du3L4lEghkzZrBs2TK++c1vctttt7FgwQL69+/f7LHamxq7oqKi01N0p1x66aX8/Oc/Z/r06dxwww386Ec/4vbbb+eWW27h/fffp7i4OD1kdeutt3LnnXcybdo0ampqKCkp6fRmbk8kewpmmjpb5GA1adIktm3bxqZNm1i6dCkVFRUMGzYMd+d73/se48eP59RTT2Xjxo1s3bq13cd56aWX0h/O48ePZ/z48en75s6dy+TJk5k0aRJvv/02K1as6LCm9qbGhs5P0Q3BZH47d+5k+vTpAFx22WW89NJL6Rovvvhifve731FQEPw/P23aNL797W9zxx13sHPnznT7xxHJnsLek9fUUxD5WDr4jz6bzjvvPB577DG2bNnCBRdcAMCDDz7I9u3bWbRoEYWFhQwfPrzNKbP35f333+fWW2/ljTfeoKKigssvv7xLj5PS2Sm69+XPf/4zL730En/84x+5+eabeeutt5gzZw5nnHEG8+fPZ9q0aTz99NMcc8wxXa4VIttTSA0faZ+CyMHoggsu4JFHHuGxxx7jvPPOA4L/sgcOHEhhYSELFixg3bp1HT7GZz7zGR566CEAli9fzrJlywDYtWsXZWVl9O7dm61bt/KXv/wlvU5703a3NzX2/urduzcVFRXpXsYDDzzA9OnTSSaTrF+/nlNOOYWf/OQnVFVVUVNTw5o1axg3bhzf/e53Oe6449KXC/04ItpTCIaPNHW2yMFpzJgxVFdXM2TIEAYNGgTAxRdfzJlnnsm4ceOYMmXKPv9jvvrqq7niiisYNWoUo0aN4thjjwVgwoQJTJo0iWOOOYZhw4Yxbdq09DqzZ89m5syZDB48mAULFqTb25sau6Ohovbcf//9fP3rX2fPnj0cccQR/PrXvyaRSHDJJZdQVVWFu/PNb36TPn368MMf/pAFCxYQi8UYM2YMp59++n4/X0tZmzo7F7o6dfbGvy9hyEPTee3YWznhzK9moTKRTy5NnX1w2d+psyM5fJQ++kiHpIqINBPJUNA0FyIibYtmKIT7FEz7FES65GAedo6SrvyeIhkK6amzNXwkst9KSkqorKxUMHRz7k5lZeV+n9AW0aOPNHwk0lVDhw5lw4YNbN++Pd+lyD6UlJQwdOjQ/VonoqGQOiRVoSCyvwoLCxkxYkS+y5AsiebwUVzDRyIibYlmKKQmyNKEeCIizUQyFCwejpqppyAi0kw0QyGmk9dERNoSyVBIXXlNoSAi0lw0QyGmy3GKiLQlkqFgsXBHs3oKIiLNRDIU4nFNnS0i0pZIhkIsffSRTtMXEckUzVDQ0UciIm3KWiiY2TAzW2BmK8zsbTP7Vtje18yeNbNV4feKsN3M7A4zW21my8xscvZq09FHIiJtyWZPoQn4Z3cfDUwFrjGz0cAc4Hl3Hwk8H/4McDowMvyaDdyVtcp0jWYRkTZlLRTcfbO7Lw5vVwPvAEOAs4H7w8XuB84Jb58N/NYDrwJ9zGxQVopLTXOhHc0iIs3kZJ+CmQ0HJgGvAYe4++bwri3AIeHtIcD6jNU2hG0tH2u2mS00s4VdnrrXjISbho9ERFrIeiiYWU/gceBad9+VeZ8HV+nYr0OA3P1ud5/i7lMGDBjQ5bqSxBQKIiItZDUUzKyQIBAedPcnwuatqWGh8Pu2sH0jMCxj9aFhW1Y46imIiLSUzaOPDLgXeMfdb8u460ngsvD2ZcAfMtovDY9CmgpUZQwzHXBJDNu/ToqIyCdeNq+8Ng34MvCWmS0J274H3ALMNbMrgXXA+eF984FZwGpgD3BFFmsjaTFdT0FEpIWshYK7vwJYO3fPaGN5B67JVj0taZ+CiEhrkTyjGbRPQUSkLZENhSQx9vPAJxGRT7zIhoJjmHoKIiLNRDYUkhimM5pFRJqJcChoR7OISEuRDQUnpuspiIi0ENlQSJr2KYiItBTZUHBigEJBRCRThEPBNHwkItJChEMhpqOPRERaiGwoJC2mfQoiIi1ENhQcQ2c0i4g0F9lQSKKegohIS5ENBU2IJyLSWnRDwWLEdEiqiEgz0Q0FDR+JiLQS2VBImqa5EBFpKbKh4Bim4SMRkWYiGwpo+EhEpJXIhoKbegoiIi1FNhR0noKISGuRDQW3GKYzmkVEmoluKKinICLSSmRDATN0PQURkeYiGwpOjJh6CiIizUQ3FLRPQUSklYiHgnoKIiKZohsKxDBNcyEi0kx0Q0Enr4mItBLZUNA0FyIirWUtFMzsPjPbZmbLM9puMrONZrYk/JqVcd/1ZrbazN41s89nq64UXU9BRKS1bPYUfgPMbKP9Z+4+MfyaD2Bmo4ELgTHhOr80s3gWa9PRRyIibchaKLj7S8CHnVz8bOARd6939/eB1cDx2aoNANPwkYhIS/nYp/ANM1sWDi9VhG1DgPUZy2wI21oxs9lmttDMFm7fvr3LRQTXU1BPQUQkU65D4S7gSGAisBn46f4+gLvf7e5T3H3KgAEDul6J9imIiLSS01Bw963unnD3JHAPe4eINgLDMhYdGrZlrxaFgohIKzkNBTMblPHjuUDqyKQngQvNrNjMRgAjgdezWYubTl4TEWmpIFsPbGYPAycD/c1sA3AjcLKZTQQcWAt8DcDd3zazucAKoAm4xt0T2aotKFDTXIiItJS1UHD3i9povreD5W8Gbs5WPa2ejxgx7WgWEWkmumc06zwFEZFWIh0K2tEsItJcZENBRx+JiLQW2VDQ8JGISGuRDgVdjlNEpLlOhYKZfcvMelngXjNbbGanZbu4rDJNcyEi0lJnewpfcfddwGlABfBl4JasVZUTOiRVRKSlzoaChd9nAQ+4+9sZbQclj8W1o1lEpIXOhsIiM3uGIBSeNrNyONg/UXX0kYhIS509o/lKgplN33P3PWbWF7gie2XlgI4+EhFppbM9hROBd919p5ldAvwAqMpeWTkQ0z4FEZGWOhsKdwF7zGwC8M/AGuC3WasqF8yIkcQ1U6qISFpnQ6HJg0/Ps4FfuPudQHn2ysoBixM3J5lUKIiIpHR2n0K1mV1PcCjqSWYWAwqzV1YOWJCHSU8Sj/A5fCIimTr7aXgBUE9wvsIWgiuj/VvWqsoBD0MhkcjuZRtERA4mnQqFMAgeBHqb2T8Cde5+UO9TsDAUPKnDUkVEUjo7zcX5BJfHPA84H3jNzL6UzcKyLjV8lFRPQUQkpbP7FL4PHOfu2wDMbADwHPBYtgrLNouFoZBoynMlIiLdR2f3KcRSgRCq3I91uyW3OABJDR+JiKR1tqfwlJk9DTwc/nwBMD87JeWIBVM3uabPFhFJ61QouPt1ZvZFYFrYdLe7z8teWdlnqZ6Cjj4SEUnrbE8Bd38ceDyLteRW2FPQ8JGIyF4dhoKZVUObEwQZ4O7eKytV5YDFgp6CJ8MdzU0NwfeCojxVJCKSfx2Ggrsf3FNZdMRSoRBm3u+vhmQTnH9/HosSEcmvTg8ffeLEUsNHwT6FHRtWUdC0mz75rElEJM8O6sNKP5b0Gc1BKOyq2U1i90f5rEhEJO8iGwrW4jyFuDfS03eBptIWkQiLbCikz1MIQ6HAGyimERr35LMqEZG8imwoWCzYnZIMjz4q9ODoo8aayrzVJCKSb5ENBWLNZ0kt9EYAqj/a1u4qIiKfdFkLBTO7z8y2mdnyjLa+Zvasma0Kv1eE7WZmd5jZajNbZmaTs1XX3lpahAJBj6G2anu2n1pEpNvKZk/hN8DMFm1zgOfdfSTwfPgzwOnAyPBrNsE1obMqHQqeCoWgp1C7a0e2n1pEpNvKWii4+0vAhy2azwZSZ4fdD5yT0f5bD7wK9DGzQdmqDUgPHyUTCXAPdjIDDQoFEYmwXO9TOMTdN4e3twCHhLeHAOszltsQtrViZrPNbKGZLdy+vetDPZZxkR1vqk+3J3ZrR7OIRFfedjS7u9P2vEr7Wu9ud5/i7lMGDBjQ5edPzX2EJ2lqrNv7+Ht0ApuIRFeuQ2Fralgo/J461GcjMCxjuaFhW9ZYbO8ZzQ11tXvbaxUKIhJduQ6FJ4HLwtuXAX/IaL80PAppKlCVMcyUHanhI0/SWL+3p1BQr1AQkejK2oR4ZvYwcDLQ38w2ADcCtwBzzexKYB1wfrj4fGAWsBrYA1yRrbr21peaJTVJY8PeUChqrMr2U4uIdFtZCwV3v6idu2a0sawD12SrlrZYLDXNRYLG+mD4qMljlDQpFEQkuiJ8RnOQh55M0NQQhMIOelOW2JXPqkRE8iqyoZB5RnNq+KjS+lLuNaBLdIpIREU2FGLh0Ud4kqYwFHYV9CdOEurVWxCRaIpsKFjGhHjJMBT2FAfnPdTt0glsIhJN0Q2FjLmPEo3BGc2NpcEJ1jU7t+atLhGRfIpsKOzd0dxEIjyj2cqDUNi9U/MfiUg0Ze2Q1O4uPXzkSTwMheJe/QCo3V2dt7pERPIpsj2FWMbJa+lQ6NkXgIa63XmrS0QknyIbCsT2XqM5Gc6SWlwehEKysbbd1UREPskiGwqxcJZU9yQ0Bddn7hEOHyUbFAoiEk2RDYXU1NmWTKSHj8p6B6HgDXvyVpeISD5FOBQyLseZqCfpRnl5RXCnho9EJKIiGwqpM5qTnoSmehoooLy0mHovTPccRESiJrKhkJo6m2QSEg00UEhBPEYdRViTegoiEk3RDYX03EcJLFFPgxUCUE8R1qSegohEU2RDIX30UTKJJRpoJAwFKyaWUCiISDRFNhQIQwFPEEs00Bj2FBqsSKEgIpEV2VBIT52ddGLJBpqsCIBGKyau4SMRiajIhkLmIamxRD1NYU+hKVZMPKlQEJFoimwoxNNnNCeIe0ZPIVZCQbI+n6WJiORNZEMhdUYzySTxZCOJWNhTiBdTqFAQkYiKcCjsvRxnPNlAIhb0FJLxEgpdoSAi0RTZUIhlHH1U4A0kwuGjRLyEIoWCiERU5EPBk06BN5KIB6Hg8RKKvCGfpYmI5E1kQ4H43uGjAm/Ew+EjLyihBPUURCSaIhsKcds7fFTkDSTje0OhiCZIJvJYnYhIfkQ2FGLxVCgkKaQRjxcHPxeWArrQjohEU2RDIfPktULPDIUeANTpOs0iEkGRDYXUjmbzBCXWCOHwUaywBICGWoWCiERPQT6e1MzWAtVAAmhy9ylm1hd4FBgOrAXOd/ePslVDau6jWCI40sgLgp5CrCgYPqpXKIhIBOWzp3CKu0909ynhz3OA5919JPB8+HPWpM9TCCe/sxah0KDhIxGJoO40fHQ2cH94+37gnGw+WSwedJIsvB6zhfsU4sXBPoVGhYKIRFC+QsGBZ8xskZnNDtsOcffN4e0twCFtrWhms81soZkt3L59e9crMAu+hZfetMLmodBUv6frjy0icpDKyz4F4NPuvtHMBgLPmtnKzDvd3c3M21rR3e8G7gaYMmVKm8t0VsKNeCI1fBTsYC4sLgOgqV6HpIpI9OSlp+DuG8Pv24B5wPHAVjMbBBB+35btOpLE0ldZS/UUUqGQaFBPQUSiJ+ehYGZlZlaeug2cBiwHngQuCxe7DPhDtmtxjHgimNIiHh6KWlgS7GhOaPhIRCIoH8NHhwDzLBjTLwAecvenzOwNYK6ZXQmsA87PdiFJjILwKmuxsKdQXBL0FHRGs4hEUc5Dwd3fAya00V4JzMhlLUliFIahEC8KegpFPcJQaFQoiEj0dKdDUnMuaTGKk8GHfzw8P6G4NAgFVyiISARFOxQw+iSDk6YLygcAUFLcg6QbaEeziERQpEPBMcoJPvz7DhwKQGFBjFqK0mc6i4hEScRDIXj5u72YvhUVAJgZ9RRhbYTCrq3r2LjkmZzWKCKSS5EOhUT48j+yPhTE926KBitOn+mcadXc73PovPPZ/M6rOatRRCSXIh0KTjDVxa6Cvs3aG6wofVJbpl5V7xI3p27e/8ITTTmpUUQklyIeCsHLry3q16y93orTJ7WlJJuaGNq4lrUMZkTD33nnmXtzVqeISK5EOhSSYU+hoaR/s/amWHF6TqSUrR+spIc1sGns16jyMna/pyEkEfnkiXQouIWX5Cwb2Ky9KVacPtM5ZeuqxQD0GzGRTQVDKd31Xm6KFBHJoUiHQjJ8+fHy5rN0N8ZK0mc6p9RvXEbSjaFHT6aqbAQD6z/IWZ0iIrkS6VBI7Wgurji0WXtDvCclyeYnrxVVrmRjbBBlPXvR1PdTDOBDaquzdrVQEZG8iHQopHoKZRWDmrU3FvehZ7K6WVv/PWvYXnokACWHHgPA5jVv5aBKEZHciXQoeHj1tT7h2cwpyeI+9GI3JBMA1O6uZkhyM/V9gzDoe/gYAKrWv53DakVEsi/aoRC+/IqBQ5q1W2lwdnNd9YcAbFu/ipg5hQNHAjB4xGgaPU7D1ndzWK2ISPZFOhSSxKihBwUlPZu1x8uCk9lqdgbXgN61bR0APfoNA6CkpIRNsUMp3rk6h9WKiGRfpEPBzaiK9WnVXtgzCIXdO4MrgtZWbgCgzyGHpZepLDmcij3rclCliEjuRDoUzOLUFvVv1V5cHrTVVu0AILFzIwD9Bg1PL1Pb+0gGJTaSbGrMfqEiIjkS6VAYOOE0Bh13dqv20j7BtRXqqysBiNVsZic9KSndO8xUMPAoiizB1g+0X0FEPjnycY3mbqPszFvabO8ZhkLT7mBHc3HtVj6K9SdzoKl86BhYBtvXvsWgI8Zmu1QRkZyIdE+hPb0rguGjZBgKPeu3satoQLNlBh05DoDaTStzW5yISBYpFNrQo7iIKi/D6oIzlvskKqkvbT4VRkW/geygN7FKHYEkIp8cCoU2mBnV1pNY3U4a6uvoz06SZYNaLbe18DDKa9qfGM83Lqb27pn4q3dBfU02SxYROSAUCu3YHS+nsKGKHZuDw05jvQe3Wqa65xEc2ri+7Qeor6HqgS8T27gQe2oOO+/7Irhns2QRkY9NodCO2ngvipt2UbUtmA21JDxxLZP3O5I+VFO1Y3Or+7Y+fh29ajfyrwP/hZ8XXUmfra+ye9kfs163iMjHoVBoR0NRH0oTu9izIwiF8oGHtVqmx+BRAGx+b1mz9mTNDvr9/VGeLJrJt6+6gs9e+j3WJAdR+5cfgC7jKSLdmEKhHU1FfShP7qLxo+DEtb6HHt5qmQHDgyOQqtevaNb+/osPUECC3p+eTVlxAWOG9ue/h3+D/nXr2PHaw9kvXkSkixQK7Uj26EO574ZdG6n3QnpVDGy1zKGHjWS3F+OblzZrL3jrUVZyOCf+w/R02+e/+BXW+GDqX75D+xZEpNtSKLTDelQQM2fgjtfYHB+ExVpvqnhBAat6TGBw5WvptuoNKzi87h3eH3wmJYXxdPshvUtZOvRihtT+nZ0rX8zJaxAR2V8KhXYUhDOlHpF4n83DZrW7XO1h0xnqm9iyLjiJ7YPnfkWTxzj85EtbLTvpH7/Gh96Tyqd/0u7jeTLJmy//iZd/cwP//ehP2fTBmo/5SkREOk+h0I6Csn7p20Onf6Xd5Q6dfAYA69/4E8n6PQxd+wSvF5/IqJFHtVp2xKABvDTgnzhy5/+wbclTre5f++5SVt48lUnPX8xJa/+dae/8mH73nsArd86maueHB+BViYh0rNuFgpnNNLN3zWy1mc3JVx0lvYJQWFY4nmFHHN3ucsOPmsAW+lPw/gu8+/z99KYaP/6rWHhVt5aOu/B7rPeBNPx5TrMjkRY/+yD9Hvo8gxKbWDzuBuq+vYbNF7/A8n6f5x+2zWXP7cez9MV57RecaKRhzUtULriTLX/8v3z40n/Q9P4r0FTftQ0gIpHUrSbEM7M4cCfwOWAD8IaZPenuKzpe88ArP/QImjxG5dEXd7icxWKs6zOViR89Te3rS3jPhnH8yWe1u/yQ/hU8Nfb/MPPt77D638+g5HPXs+F/HmPq5gdYXfgpel32MJOHBb2MQb36M2jkg6xa+BzF87/JhAWX8/riR+g7/escOXoyjXt2sfWtBdSt+AuDtr9Mz2Q1/Vo8X8TNnsgAAAk7SURBVD3FbOo1gabDpjFg3Ax6Dx2NlfYlmUyye1clNR9uobpyC7U7t9JUV0OsoIhYYTEFxWWU9u5Hr4qBlFcMpKisD8RikExCogGa6qCxlmRDLY31u2lsqMUsRixWQKygAIsVEC8oJBaPY7ECiBWAxSGW+mrR1k6IikhumXejI2HM7ETgJnf/fPjz9QDu/i9tLT9lyhRfuHBh1up59a2/c+zoT1EY77hD9e6br1D91I9JJhI0TL6KT8+6qMPlGxNJ5v/6/zFz/c8otqC38HrFGYyffQ8lPcraXKduTw1LHpjD5E0PU2TNz3Wo9HLeKDqeqmGnUnbkVErK+7Jn5zZq1y2idNP/MHLPmxxj7Zx53UkJDCdGAYmP9TjtP36MZNhxNTLfk229Pw0Pl3Ks2Roe3tdV+1r34zz2x2dhDbmTq9ebi+cxwHP068vF61k94mKmXNr2TM/7YmaL3H1Km/d1s1D4EjDT3a8Kf/4ycIK7fyNjmdnAbIDDDjvs2HXrDt6rny1d/Bq7PljKsFEnMPzoCZ1ap+qjSla8OJeGjzbTGCuk6PCpjJxwIoMqera7Tl1jgpVr3mP7ipdh5zoKG3ZSVFCA9eiDlfWnpPdASvscQnHP3iQa62mqr6O+rob6XZXU13xIYveH+J6P8GQTyVgRCSsgGS/GC3tgBT2gsAQrKAEckk14MgHJJvAkJJowbwJPQCIBnsCS4XdPYN6EJVO3EyQ9+IBPNnsFmX9gjrmnPx4NTx/iu++PzI7f6+loaXexrj/2AZPTP9dsP1nm7y37z2Q5+6zLzfOUHD2DE2Zd1qV1P1GhkCnbPQURkU+ijkKhu+1o3ghkTjI0NGwTEZEc6G6h8AYw0sxGmFkRcCHwZJ5rEhGJjG519JG7N5nZN4CngThwn7u/neeyREQio1uFAoC7zwfm57sOEZEo6m7DRyIikkcKBRERSVMoiIhImkJBRETSutXJa/vLzLYDXTmluT+w4wCXcyCorv3XXWtTXfunu9YF3be2j1PX4e4+oK07DupQ6CozW9je2Xz5pLr2X3etTXXtn+5aF3Tf2rJVl4aPREQkTaEgIiJpUQ2Fu/NdQDtU1/7rrrWprv3TXeuC7ltbVuqK5D4FERFpW1R7CiIi0gaFgoiIpEUqFMxsppm9a2arzWxOnmsZZmYLzGyFmb1tZt8K228ys41mtiT8mpWH2taa2Vvh8y8M2/qa2bNmtir8XpHjmo7O2CZLzGyXmV2br+1lZveZ2TYzW57R1uY2ssAd4ftumZlNznFd/2ZmK8PnnmdmfcL24WZWm7HtfpXjutr93ZnZ9eH2etfMPp/juh7NqGmtmS0J23O5vdr7fMj+e8zdI/FFMBX3GuAIoAhYCozOYz2DgMnh7XLg78Bo4CbgO3neVmuB/i3a/hWYE96eA/wkz7/LLcDh+dpewGeAycDyfW0jYBbwF4IrT04FXstxXacBBeHtn2TUNTxzuTxsrzZ/d+HfwVKgGBgR/t3Gc1VXi/t/CtyQh+3V3udD1t9jUeopHA+sdvf33L0BeAQ4O1/FuPtmd18c3q4G3gGG5KueTjgbuD+8fT9wTh5rmQGscfe8XaDb3V8CPmzR3N42Ohv4rQdeBfqY2aBc1eXuz7h7U/jjqwRXNMypdrZXe84GHnH3end/H1hN8Peb07rMzIDzgYez8dwd6eDzIevvsSiFwhBgfcbPG+gmH8JmNhyYBLwWNn0j7ALel+thmpADz5jZIjObHbYd4u6bw9tbgEPyUFfKhTT/Q8339kppbxt1p/feVwj+o0wZYWZvmtmLZnZSHupp63fXXbbXScBWd1+V0Zbz7dXi8yHr77EohUK3ZGY9gceBa919F3AXcCQwEdhM0H3NtU+7+2TgdOAaM/tM5p0e9FfzciyzBZdpPQv4r7CpO2yvVvK5jdpjZt8HmoAHw6bNwGHuPgn4NvCQmfXKYUnd8neX4SKa//OR8+3VxudDWrbeY1EKhY3AsIyfh4ZteWNmhQS/8Afd/QkAd9/q7gl3TwL3kKVuc0fcfWP4fRswL6xha6o7Gn7fluu6QqcDi919a1hj3rdXhva2Ud7fe2Z2OfCPwMXhhwnh8ExleHsRwdj9UbmqqYPfXXfYXgXAF4BHU2253l5tfT6Qg/dYlELhDWCkmY0I/9u8EHgyX8WE45X3Au+4+20Z7ZnjgOcCy1uum+W6ysysPHWbYCflcoJtdVm42GXAH3JZV4Zm/73le3u10N42ehK4NDxCZCpQlTEEkHVmNhP4P8BZ7r4no32AmcXD20cAI4H3clhXe7+7J4ELzazYzEaEdb2eq7pCpwIr3X1DqiGX26u9zwdy8R7LxZ707vJFsIf+7wQJ//081/Jpgq7fMmBJ+DULeAB4K2x/EhiU47qOIDjyYynwdmo7Af2A54FVwHNA3zxsszKgEuid0ZaX7UUQTJuBRoLx2yvb20YER4TcGb7v3gKm5Liu1QTjzan32a/CZb8Y/o6XAIuBM3NcV7u/O+D74fZ6Fzg9l3WF7b8Bvt5i2Vxur/Y+H7L+HtM0FyIikhal4SMREdkHhYKIiKQpFEREJE2hICIiaQoFERFJUyiI5ImZnWxmf8p3HSKZFAoiIpKmUBDZBzO7xMxeD+fQ/w8zi5tZjZn9LJzr/nkzGxAuO9HMXrW91y5IzXf/KTN7zsyWmtliMzsyfPieZvaYBdc7eDA8k1UkbxQKIh0ws1HABcA0d58IJICLCc6uXujuY4AXgRvDVX4LfNfdxxOcWZpqfxC4090nAP9AcBYtBLNfXkswV/4RwLSsvyiRDhTkuwCRbm4GcCzwRvhPfA+CSciS7J0s7XfAE2bWG+jj7i+G7fcD/xXOJTXE3ecBuHsdQPh4r3s4v44FV/gaDryS/Zcl0jaFgkjHDLjf3a9v1mj2wxbLdXW+mPqM2wn0Nyl5puEjkY49D3zJzAZC+hq5hxP87XwpXOafgFfcvQr4KOPiK18GXvTgylkbzOyc8DGKzaw0p69CpJP0X4lIB9x9hZn9gOBKdDGC2TSvAXYDx4f3bSPY7wDBdMa/Cj/03wOuCNu/DPyHmf04fIzzcvgyRDpNs6SKdIGZ1bh7z3zXIXKgafhIRETS1FMQEZE09RRERCRNoSAiImkKBRERSVMoiIhImkJBRETS/j/7j5bkZb6/MAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Best epoch number is:  199\n",
            "The lowest validation loss is:  0.34599860543317285\n",
            "At the same epoch, the training loss is:  0.11686281285391474\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNl0Elu65gg1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8e5defbb-00b0-4522-9e29-9f7c93fbe1ef"
      },
      "source": [
        "print(\"MMSE\")\n",
        "mmse_train_pred, mmse_val_pred, tr_losses, te_losses=NN(x_train,y_train_mmse,x_val,y_val_mmse)\n"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MMSE\n",
            "Iter 0, training loss 201.368759, validation loss 201.463226\n",
            "Iter 1, training loss 236.445618, validation loss 242.682083\n",
            "Iter 2, training loss 298.875641, validation loss 309.268311\n",
            "Iter 3, training loss 219.214691, validation loss 230.948700\n",
            "Iter 4, training loss 124.478279, validation loss 134.715302\n",
            "Iter 5, training loss 74.982796, validation loss 82.338074\n",
            "Iter 6, training loss 64.825340, validation loss 69.583763\n",
            "Iter 7, training loss 68.310013, validation loss 71.587006\n",
            "Iter 8, training loss 66.691483, validation loss 69.619141\n",
            "Iter 9, training loss 56.755756, validation loss 60.044266\n",
            "Iter 10, training loss 47.996151, validation loss 52.145168\n",
            "Iter 11, training loss 50.164635, validation loss 55.646175\n",
            "Iter 12, training loss 53.880840, validation loss 60.989750\n",
            "Iter 13, training loss 45.377167, validation loss 54.074478\n",
            "Iter 14, training loss 32.856354, validation loss 42.843212\n",
            "Iter 15, training loss 26.688602, validation loss 37.615650\n",
            "Iter 16, training loss 26.498976, validation loss 38.164181\n",
            "Iter 17, training loss 26.168484, validation loss 38.603035\n",
            "Iter 18, training loss 22.693390, validation loss 36.061588\n",
            "Iter 19, training loss 18.171585, validation loss 32.659328\n",
            "Iter 20, training loss 16.436005, validation loss 32.069099\n",
            "Iter 21, training loss 18.121309, validation loss 34.698868\n",
            "Iter 22, training loss 19.212290, validation loss 36.290863\n",
            "Iter 23, training loss 17.098501, validation loss 34.173435\n",
            "Iter 24, training loss 14.105151, validation loss 30.825876\n",
            "Iter 25, training loss 13.074822, validation loss 29.336971\n",
            "Iter 26, training loss 13.616906, validation loss 29.516396\n",
            "Iter 27, training loss 13.495952, validation loss 29.216202\n",
            "Iter 28, training loss 11.863758, validation loss 27.539869\n",
            "Iter 29, training loss 10.088626, validation loss 25.749016\n",
            "Iter 30, training loss 9.796075, validation loss 25.375721\n",
            "Iter 31, training loss 10.536342, validation loss 25.942991\n",
            "Iter 32, training loss 10.426924, validation loss 25.633368\n",
            "Iter 33, training loss 9.219357, validation loss 24.296633\n",
            "Iter 34, training loss 8.426258, validation loss 23.492914\n",
            "Iter 35, training loss 8.638459, validation loss 23.797161\n",
            "Iter 36, training loss 8.845943, validation loss 24.153433\n",
            "Iter 37, training loss 8.234770, validation loss 23.698887\n",
            "Iter 38, training loss 7.309460, validation loss 22.905960\n",
            "Iter 39, training loss 6.977911, validation loss 22.684313\n",
            "Iter 40, training loss 7.135056, validation loss 22.967346\n",
            "Iter 41, training loss 6.925812, validation loss 22.943781\n",
            "Iter 42, training loss 6.278555, validation loss 22.544668\n",
            "Iter 43, training loss 5.909903, validation loss 22.435612\n",
            "Iter 44, training loss 6.004260, validation loss 22.747555\n",
            "Iter 45, training loss 6.029684, validation loss 22.922632\n",
            "Iter 46, training loss 5.702136, validation loss 22.675356\n",
            "Iter 47, training loss 5.366810, validation loss 22.366066\n",
            "Iter 48, training loss 5.337219, validation loss 22.344032\n",
            "Iter 49, training loss 5.367282, validation loss 22.408630\n",
            "Iter 50, training loss 5.137893, validation loss 22.264109\n",
            "Iter 51, training loss 4.826242, validation loss 22.068493\n",
            "Iter 52, training loss 4.720526, validation loss 22.071526\n",
            "Iter 53, training loss 4.715569, validation loss 22.140726\n",
            "Iter 54, training loss 4.574648, validation loss 22.030958\n",
            "Iter 55, training loss 4.357564, validation loss 21.809036\n",
            "Iter 56, training loss 4.270474, validation loss 21.701157\n",
            "Iter 57, training loss 4.276872, validation loss 21.704050\n",
            "Iter 58, training loss 4.187954, validation loss 21.654648\n",
            "Iter 59, training loss 4.025806, validation loss 21.572727\n",
            "Iter 60, training loss 3.944842, validation loss 21.586454\n",
            "Iter 61, training loss 3.922806, validation loss 21.643833\n",
            "Iter 62, training loss 3.834335, validation loss 21.601231\n",
            "Iter 63, training loss 3.700777, validation loss 21.479719\n",
            "Iter 64, training loss 3.629861, validation loss 21.404800\n",
            "Iter 65, training loss 3.601426, validation loss 21.386627\n",
            "Iter 66, training loss 3.525650, validation loss 21.356976\n",
            "Iter 67, training loss 3.428961, validation loss 21.338055\n",
            "Iter 68, training loss 3.382019, validation loss 21.373436\n",
            "Iter 69, training loss 3.352192, validation loss 21.401983\n",
            "Iter 70, training loss 3.283490, validation loss 21.354158\n",
            "Iter 71, training loss 3.208397, validation loss 21.270723\n",
            "Iter 72, training loss 3.168006, validation loss 21.220007\n",
            "Iter 73, training loss 3.127535, validation loss 21.197201\n",
            "Iter 74, training loss 3.059418, validation loss 21.185667\n",
            "Iter 75, training loss 2.999997, validation loss 21.205515\n",
            "Iter 76, training loss 2.964931, validation loss 21.242958\n",
            "Iter 77, training loss 2.921428, validation loss 21.244516\n",
            "Iter 78, training loss 2.864697, validation loss 21.204592\n",
            "Iter 79, training loss 2.822752, validation loss 21.169945\n",
            "Iter 80, training loss 2.790942, validation loss 21.161697\n",
            "Iter 81, training loss 2.746094, validation loss 21.170921\n",
            "Iter 82, training loss 2.698793, validation loss 21.199572\n",
            "Iter 83, training loss 2.664654, validation loss 21.239202\n",
            "Iter 84, training loss 2.629774, validation loss 21.255049\n",
            "Iter 85, training loss 2.586545, validation loss 21.235088\n",
            "Iter 86, training loss 2.549726, validation loss 21.207607\n",
            "Iter 87, training loss 2.520193, validation loss 21.194378\n",
            "Iter 88, training loss 2.485183, validation loss 21.195549\n",
            "Iter 89, training loss 2.448923, validation loss 21.210262\n",
            "Iter 90, training loss 2.419789, validation loss 21.228691\n",
            "Iter 91, training loss 2.390103, validation loss 21.227190\n",
            "Iter 92, training loss 2.356021, validation loss 21.199720\n",
            "Iter 93, training loss 2.325537, validation loss 21.166561\n",
            "Iter 94, training loss 2.298156, validation loss 21.144995\n",
            "Iter 95, training loss 2.267647, validation loss 21.138050\n",
            "Iter 96, training loss 2.237833, validation loss 21.143288\n",
            "Iter 97, training loss 2.211980, validation loss 21.148781\n",
            "Iter 98, training loss 2.185176, validation loss 21.138245\n",
            "Iter 99, training loss 2.157304, validation loss 21.112467\n",
            "Iter 100, training loss 2.132345, validation loss 21.087675\n",
            "Iter 101, training loss 2.107882, validation loss 21.074469\n",
            "Iter 102, training loss 2.081847, validation loss 21.074041\n",
            "Iter 103, training loss 2.057406, validation loss 21.080893\n",
            "Iter 104, training loss 2.034294, validation loss 21.082296\n",
            "Iter 105, training loss 2.010187, validation loss 21.070105\n",
            "Iter 106, training loss 1.986838, validation loss 21.051115\n",
            "Iter 107, training loss 1.965118, validation loss 21.037384\n",
            "Iter 108, training loss 1.942957, validation loss 21.034245\n",
            "Iter 109, training loss 1.920978, validation loss 21.039774\n",
            "Iter 110, training loss 1.900339, validation loss 21.045198\n",
            "Iter 111, training loss 1.879618, validation loss 21.041237\n",
            "Iter 112, training loss 1.858861, validation loss 21.028530\n",
            "Iter 113, training loss 1.839172, validation loss 21.016161\n",
            "Iter 114, training loss 1.819749, validation loss 21.011463\n",
            "Iter 115, training loss 1.800237, validation loss 21.014822\n",
            "Iter 116, training loss 1.781589, validation loss 21.020544\n",
            "Iter 117, training loss 1.763348, validation loss 21.020538\n",
            "Iter 118, training loss 1.745022, validation loss 21.012594\n",
            "Iter 119, training loss 1.727306, validation loss 21.002558\n",
            "Iter 120, training loss 1.709972, validation loss 20.997183\n",
            "Iter 121, training loss 1.692602, validation loss 20.998201\n",
            "Iter 122, training loss 1.675719, validation loss 21.001873\n",
            "Iter 123, training loss 1.659246, validation loss 21.002174\n",
            "Iter 124, training loss 1.642856, validation loss 20.996443\n",
            "Iter 125, training loss 1.626874, validation loss 20.988552\n",
            "Iter 126, training loss 1.611271, validation loss 20.984203\n",
            "Iter 127, training loss 1.595750, validation loss 20.985569\n",
            "Iter 128, training loss 1.580566, validation loss 20.989624\n",
            "Iter 129, training loss 1.565694, validation loss 20.991205\n",
            "Iter 130, training loss 1.550912, validation loss 20.988188\n",
            "Iter 131, training loss 1.536425, validation loss 20.983654\n",
            "Iter 132, training loss 1.522204, validation loss 20.982290\n",
            "Iter 133, training loss 1.508103, validation loss 20.985556\n",
            "Iter 134, training loss 1.494313, validation loss 20.990602\n",
            "Iter 135, training loss 1.480801, validation loss 20.993212\n",
            "Iter 136, training loss 1.467437, validation loss 20.992031\n",
            "Iter 137, training loss 1.454344, validation loss 20.989557\n",
            "Iter 138, training loss 1.441466, validation loss 20.989332\n",
            "Iter 139, training loss 1.428754, validation loss 20.992178\n",
            "Iter 140, training loss 1.416286, validation loss 20.995756\n",
            "Iter 141, training loss 1.404020, validation loss 20.996897\n",
            "Iter 142, training loss 1.391893, validation loss 20.995192\n",
            "Iter 143, training loss 1.379980, validation loss 20.993183\n",
            "Iter 144, training loss 1.368237, validation loss 20.993570\n",
            "Iter 145, training loss 1.356643, validation loss 20.996197\n",
            "Iter 146, training loss 1.345257, validation loss 20.998880\n",
            "Iter 147, training loss 1.334038, validation loss 20.999565\n",
            "Iter 148, training loss 1.322996, validation loss 20.998804\n",
            "Iter 149, training loss 1.312129, validation loss 20.999016\n",
            "Iter 150, training loss 1.301421, validation loss 21.001785\n",
            "Iter 151, training loss 1.290881, validation loss 21.006264\n",
            "Iter 152, training loss 1.280512, validation loss 21.010225\n",
            "Iter 153, training loss 1.270297, validation loss 21.012592\n",
            "Iter 154, training loss 1.260264, validation loss 21.014380\n",
            "Iter 155, training loss 1.250397, validation loss 21.017483\n",
            "Iter 156, training loss 1.240658, validation loss 21.022305\n",
            "Iter 157, training loss 1.231056, validation loss 21.027344\n",
            "Iter 158, training loss 1.221602, validation loss 21.031042\n",
            "Iter 159, training loss 1.212300, validation loss 21.033449\n",
            "Iter 160, training loss 1.203146, validation loss 21.036013\n",
            "Iter 161, training loss 1.194131, validation loss 21.039833\n",
            "Iter 162, training loss 1.185268, validation loss 21.044735\n",
            "Iter 163, training loss 1.176543, validation loss 21.049345\n",
            "Iter 164, training loss 1.167947, validation loss 21.052807\n",
            "Iter 165, training loss 1.159475, validation loss 21.055584\n",
            "Iter 166, training loss 1.151124, validation loss 21.058819\n",
            "Iter 167, training loss 1.142888, validation loss 21.062935\n",
            "Iter 168, training loss 1.134789, validation loss 21.067228\n",
            "Iter 169, training loss 1.126804, validation loss 21.070507\n",
            "Iter 170, training loss 1.118929, validation loss 21.072737\n",
            "Iter 171, training loss 1.111163, validation loss 21.075031\n",
            "Iter 172, training loss 1.103513, validation loss 21.078253\n",
            "Iter 173, training loss 1.095995, validation loss 21.081835\n",
            "Iter 174, training loss 1.088570, validation loss 21.084837\n",
            "Iter 175, training loss 1.081230, validation loss 21.087122\n",
            "Iter 176, training loss 1.073984, validation loss 21.089693\n",
            "Iter 177, training loss 1.066822, validation loss 21.093256\n",
            "Iter 178, training loss 1.059754, validation loss 21.097706\n",
            "Iter 179, training loss 1.052777, validation loss 21.102304\n",
            "Iter 180, training loss 1.045891, validation loss 21.106712\n",
            "Iter 181, training loss 1.039090, validation loss 21.111202\n",
            "Iter 182, training loss 1.032383, validation loss 21.116262\n",
            "Iter 183, training loss 1.025760, validation loss 21.121906\n",
            "Iter 184, training loss 1.019221, validation loss 21.127518\n",
            "Iter 185, training loss 1.012758, validation loss 21.132576\n",
            "Iter 186, training loss 1.006370, validation loss 21.137222\n",
            "Iter 187, training loss 1.000072, validation loss 21.141870\n",
            "Iter 188, training loss 0.993852, validation loss 21.146749\n",
            "Iter 189, training loss 0.987719, validation loss 21.151630\n",
            "Iter 190, training loss 0.981675, validation loss 21.156137\n",
            "Iter 191, training loss 0.975707, validation loss 21.160326\n",
            "Iter 192, training loss 0.969808, validation loss 21.164461\n",
            "Iter 193, training loss 0.963978, validation loss 21.168673\n",
            "Iter 194, training loss 0.958221, validation loss 21.172842\n",
            "Iter 195, training loss 0.952537, validation loss 21.176975\n",
            "Iter 196, training loss 0.946917, validation loss 21.181097\n",
            "Iter 197, training loss 0.941372, validation loss 21.185497\n",
            "Iter 198, training loss 0.935898, validation loss 21.190077\n",
            "Iter 199, training loss 0.930492, validation loss 21.194647\n",
            "Size of the model predictions for training is:  (6716, 1)\n",
            "Size of the model predictions for validation is:  (2238, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cGf-SGI5gg6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "outputId": "7267a028-350e-48d1-9bf6-f1182b9ea82b"
      },
      "source": [
        "tr_loss, te_loss=plot_NN(epochs, tr_losses, te_losses)\n",
        "print(\"Best epoch number is: \", te_loss.index(min(te_loss)))\n",
        "print(\"The lowest validation loss is: \", min(te_loss))\n",
        "print(\"At the same epoch, the training loss is: \", tr_loss[te_loss.index(min(te_loss))])"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Best epoch number is:  132\n",
            "The lowest validation loss is:  4.580642997216039\n",
            "At the same epoch, the training loss is:  1.2337763336525054\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CKWH24CLYFuJ"
      },
      "source": [
        "# Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zi4wyUA6uzez",
        "colab": {}
      },
      "source": [
        "x_train_diag=x_train\n",
        "x_val_diag=x_val\n",
        "\n",
        "# Append the predicted numerical variables (ADAS13, Ventricles_Norm, MMSE) as new features for diagnosis predictions\n",
        "x_train_diag['ADAS13']=adas_train_pred\n",
        "x_train_diag['Ventricles_Norm']=ventricles_train_pred\n",
        "x_train_diag['MMSE']=mmse_train_pred\n",
        "\n",
        "y_train_diag = y_train_diag1.values.reshape(-1,1)\n",
        "\n",
        "# Same for validation data\n",
        "x_val_diag['ADAS13']=adas_val_pred\n",
        "x_val_diag['Ventricles_Norm']=ventricles_val_pred\n",
        "x_val_diag['MMSE']=mmse_val_pred\n",
        "\n",
        "y_val_diag = y_val_diag1.values.reshape(-1,1)"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WnyWGWatgBVA",
        "colab": {}
      },
      "source": [
        "def transform(y_val,predictions):\n",
        "    from sklearn.preprocessing import OneHotEncoder\n",
        "    ohe = OneHotEncoder()\n",
        "    y_val_cls = ohe.fit_transform(y_val.reshape(-1,1)).toarray()\n",
        "\n",
        "    predictions_cls = ohe.fit_transform(predictions.reshape(-1,1)).toarray()\n",
        "    cn_cls = y_val_cls[:,0]\n",
        "    mci_cls = y_val_cls[:,1]\n",
        "    ad_cls = y_val_cls[:,2]\n",
        "\n",
        "    cn_pred = predictions_cls[:,0]\n",
        "    mci_pred = predictions_cls[:,1]\n",
        "    ad_pred = predictions_cls[:,2]\n",
        "    return cn_cls,mci_cls,ad_cls,cn_pred,mci_pred,ad_pred\n",
        "\n",
        "def metrics(DX,model,X_test,y_test,predictions):\n",
        "    from sklearn.metrics import accuracy_score,precision_score,recall_score,roc_auc_score\n",
        "    pred_prob = model.predict_proba(X_test)[:,1]\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "    precision=precision_score(y_test, predictions)\n",
        "    recall=recall_score(y_test, predictions)\n",
        "    roc=roc_auc_score(y_test,pred_prob)\n",
        "    print(\"%s Accuracy: %.2f%% \" % (DX,accuracy *100))\n",
        "    print(\"%s Precision: %.2f%% \" % (DX,precision *100))\n",
        "    print(\"%s Recall: %.2f%% \" % (DX,recall * 100))\n",
        "    print(\"%s AUC: %.2f%% \" % (DX,roc *100))\n",
        "    return  "
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5Zu1C-mCYFuP"
      },
      "source": [
        "###Suport Vector Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "p1kkR7OrYFuP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3794134a-90fd-40d4-9890-cc847493399b"
      },
      "source": [
        "svc = SVC(probability=True)\n",
        "svc.fit(x_train_diag,y_train_diag)\n",
        "scores = cross_val_score(svc, x_train_diag, y_train_diag, cv=5)\n",
        "print('Training Accuracy: %0.2f (+/- %0.2f)' % (scores.mean(), scores.std()*2))\n",
        "y_validation_pred = svc.predict(x_val_diag)\n",
        "\n",
        "acc = accuracy_score(y_validation_pred,y_val_diag)\n",
        "print('Validation Accuracy: %0.2f (+/- %0.2f)' % (acc.mean(), acc.std()*2))"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy: 0.75 (+/- 0.04)\n",
            "Validation Accuracy: 0.61 (+/- 0.00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UdSzXSYqg7VW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "d9566278-0d58-4fad-9ed0-fdbb8a18317f"
      },
      "source": [
        "cn_cls,mci_cls,ad_cls,cn_pred,mci_pred,ad_pred = transform(y_val_diag,y_validation_pred)\n",
        "metrics('CN_Diag',svc,x_val_diag,cn_cls,cn_pred)\n",
        "print('*'*30)\n",
        "metrics('MCI_Diag',svc,x_val_diag,mci_cls,mci_pred)\n",
        "print('*'*30)\n",
        "metrics('AD_Diag',svc,x_val_diag,ad_cls,ad_pred)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CN_Diag Accuracy: 74.71% \n",
            "CN_Diag Precision: 73.01% \n",
            "CN_Diag Recall: 50.72% \n",
            "CN_Diag AUC: 33.25% \n",
            "******************************\n",
            "MCI_Diag Accuracy: 64.16% \n",
            "MCI_Diag Precision: 56.15% \n",
            "MCI_Diag Recall: 80.27% \n",
            "MCI_Diag AUC: 72.68% \n",
            "******************************\n",
            "AD_Diag Accuracy: 82.22% \n",
            "AD_Diag Precision: 56.51% \n",
            "AD_Diag Recall: 35.10% \n",
            "AD_Diag AUC: 39.36% \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "D5RpeSYiYFuj"
      },
      "source": [
        "###Neural Network for Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "M_HbGf1D24WW",
        "colab": {}
      },
      "source": [
        "x_train = x_train_diag\n",
        "y_train = y_train_diag2.values.reshape(-1,3)\n",
        "\n",
        "x_val = x_val_diag\n",
        "y_val = y_val_diag2.values.reshape(-1,3)"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "W3276U3TYFuk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "df913028-f350-46ba-9bdd-42d7e8a792d7"
      },
      "source": [
        "n_input = x_train.shape[1]\n",
        "n_hidden1 = 128\n",
        "n_hidden2 = 512\n",
        "n_hidden3 = 1024\n",
        "n_output = 3\n",
        "learning_rate = 0.001\n",
        "epochs = 100    #100,200,100\n",
        "batch_size = 32  #100,32,25\n",
        "\n",
        "x = tf.placeholder(tf.float32,[None,n_input])\n",
        "y_gt = tf.placeholder(tf.float32,[None,n_output])\n",
        "\n",
        "initializer = tf.contrib.layers.variance_scaling_initializer(factor=2.0, mode='FAN_IN', uniform=False)\n",
        "W1 = tf.Variable(initializer([n_input,n_hidden1]))\n",
        "b1 = tf.Variable(tf.constant(0.1,shape=[n_hidden1]))\n",
        "H1 = tf.nn.relu(tf.matmul(x,W1)+b1)\n",
        "\n",
        "W2 = tf.Variable(initializer([n_hidden1,n_hidden2]))\n",
        "b2 = tf.Variable(tf.constant(0.1,shape=[n_hidden2]))\n",
        "H2 = tf.nn.relu(tf.matmul(H1,W2)+b2)\n",
        "\n",
        "W3 = tf.Variable(initializer([n_hidden2,n_hidden3]))\n",
        "b3 = tf.Variable(tf.constant(0.1,shape=[n_hidden3]))\n",
        "H3 = tf.nn.relu(tf.matmul(H2,W3)+b3)\n",
        "\n",
        "W_out = tf.Variable(initializer([n_hidden3,n_output]))\n",
        "b_out = tf.Variable(tf.constant(0.1,shape=[n_output]))\n",
        "y_pred = tf.matmul(H3,W_out)+b_out\n",
        "\n",
        "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_gt,logits=y_pred))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "train_step = optimizer.minimize(loss)\n",
        "\n",
        "correct_prediction = tf.equal(tf.argmax(y_gt,axis=1),tf.argmax(y_pred,axis=1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "sess = tf.InteractiveSession()\n",
        "tf.global_variables_initializer().run()\n",
        "tr_losses, te_losses = [], []\n",
        "for iter in range(epochs):\n",
        "\n",
        "    sess.run(train_step, feed_dict={x:x_train, y_gt:y_train})\n",
        "    if iter%1 == 0:\n",
        "        \n",
        "        train_loss = sess.run(loss, feed_dict={x:x_train, y_gt:y_train})\n",
        "        train_acc = sess.run(accuracy, feed_dict={x:x_train, y_gt:y_train})\n",
        "        tr_losses.append(train_acc)\n",
        "        print(\"Iter %d, training loss %f, training accuracy %f\" % (iter, train_loss, train_acc))\n",
        "        \n",
        "        y_validation_pred = sess.run(y_pred,feed_dict={x:x_val})\n",
        "        val_loss = sess.run(loss,feed_dict={x:x_val, y_gt:y_val})\n",
        "        val_acc = sess.run(accuracy,feed_dict={x:x_val, y_gt:y_val})\n",
        "        te_losses.append(val_acc)\n",
        "        print(\"Iter %d, validation loss %f, validation accuracy %f\" % (iter, val_loss, val_acc))\n",
        "\n",
        "y_validation_pred = np.argmax(sess.run(y_pred,feed_dict={x:x_val}),axis=1)"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iter 0, training loss 13.096328, training accuracy 0.482728\n",
            "Iter 0, validation loss 12.412098, validation accuracy 0.352100\n",
            "Iter 1, training loss 9.717851, training accuracy 0.500596\n",
            "Iter 1, validation loss 9.435840, validation accuracy 0.372207\n",
            "Iter 2, training loss 3.083552, training accuracy 0.448779\n",
            "Iter 2, validation loss 4.044086, validation accuracy 0.296247\n",
            "Iter 3, training loss 4.107028, training accuracy 0.512507\n",
            "Iter 3, validation loss 6.027788, validation accuracy 0.437891\n",
            "Iter 4, training loss 6.142488, training accuracy 0.477963\n",
            "Iter 4, validation loss 8.356814, validation accuracy 0.436550\n",
            "Iter 5, training loss 4.642525, training accuracy 0.491811\n",
            "Iter 5, validation loss 6.938745, validation accuracy 0.436550\n",
            "Iter 6, training loss 1.625213, training accuracy 0.610036\n",
            "Iter 6, validation loss 3.602774, validation accuracy 0.446381\n",
            "Iter 7, training loss 2.664651, training accuracy 0.477219\n",
            "Iter 7, validation loss 3.832505, validation accuracy 0.414656\n",
            "Iter 8, training loss 2.802613, training accuracy 0.496724\n",
            "Iter 8, validation loss 3.527617, validation accuracy 0.417337\n",
            "Iter 9, training loss 1.684507, training accuracy 0.544521\n",
            "Iter 9, validation loss 2.461738, validation accuracy 0.429401\n",
            "Iter 10, training loss 1.235972, training accuracy 0.673466\n",
            "Iter 10, validation loss 3.325582, validation accuracy 0.411528\n",
            "Iter 11, training loss 1.938408, training accuracy 0.561197\n",
            "Iter 11, validation loss 4.464942, validation accuracy 0.400804\n",
            "Iter 12, training loss 2.061601, training accuracy 0.582639\n",
            "Iter 12, validation loss 4.445588, validation accuracy 0.414209\n",
            "Iter 13, training loss 1.696768, training accuracy 0.621501\n",
            "Iter 13, validation loss 3.810873, validation accuracy 0.434316\n",
            "Iter 14, training loss 1.044950, training accuracy 0.686123\n",
            "Iter 14, validation loss 2.934160, validation accuracy 0.476765\n",
            "Iter 15, training loss 0.561309, training accuracy 0.798988\n",
            "Iter 15, validation loss 2.121679, validation accuracy 0.504021\n",
            "Iter 16, training loss 0.774682, training accuracy 0.716498\n",
            "Iter 16, validation loss 1.950525, validation accuracy 0.511618\n",
            "Iter 17, training loss 1.120482, training accuracy 0.637582\n",
            "Iter 17, validation loss 2.281797, validation accuracy 0.466488\n",
            "Iter 18, training loss 1.048550, training accuracy 0.651429\n",
            "Iter 18, validation loss 2.399947, validation accuracy 0.479893\n",
            "Iter 19, training loss 0.722622, training accuracy 0.745831\n",
            "Iter 19, validation loss 2.413970, validation accuracy 0.522788\n",
            "Iter 20, training loss 0.585213, training accuracy 0.814920\n",
            "Iter 20, validation loss 2.739695, validation accuracy 0.501341\n",
            "Iter 21, training loss 0.694024, training accuracy 0.773973\n",
            "Iter 21, validation loss 3.168062, validation accuracy 0.492851\n",
            "Iter 22, training loss 0.766442, training accuracy 0.733026\n",
            "Iter 22, validation loss 3.317159, validation accuracy 0.480786\n",
            "Iter 23, training loss 0.661731, training accuracy 0.757296\n",
            "Iter 23, validation loss 3.084512, validation accuracy 0.483467\n",
            "Iter 24, training loss 0.466739, training accuracy 0.818344\n",
            "Iter 24, validation loss 2.582758, validation accuracy 0.489723\n",
            "Iter 25, training loss 0.338699, training accuracy 0.874628\n",
            "Iter 25, validation loss 2.054154, validation accuracy 0.514298\n",
            "Iter 26, training loss 0.353433, training accuracy 0.861674\n",
            "Iter 26, validation loss 1.769729, validation accuracy 0.523235\n",
            "Iter 27, training loss 0.437358, training accuracy 0.826831\n",
            "Iter 27, validation loss 1.736183, validation accuracy 0.526363\n",
            "Iter 28, training loss 0.456723, training accuracy 0.817004\n",
            "Iter 28, validation loss 1.769132, validation accuracy 0.517873\n",
            "Iter 29, training loss 0.388018, training accuracy 0.851102\n",
            "Iter 29, validation loss 1.806068, validation accuracy 0.512511\n",
            "Iter 30, training loss 0.313803, training accuracy 0.886242\n",
            "Iter 30, validation loss 1.889529, validation accuracy 0.503575\n",
            "Iter 31, training loss 0.295709, training accuracy 0.896069\n",
            "Iter 31, validation loss 2.039947, validation accuracy 0.495085\n",
            "Iter 32, training loss 0.315517, training accuracy 0.873585\n",
            "Iter 32, validation loss 2.186681, validation accuracy 0.489723\n",
            "Iter 33, training loss 0.322846, training accuracy 0.862865\n",
            "Iter 33, validation loss 2.242563, validation accuracy 0.499106\n",
            "Iter 34, training loss 0.294804, training accuracy 0.878350\n",
            "Iter 34, validation loss 2.189237, validation accuracy 0.503128\n",
            "Iter 35, training loss 0.246207, training accuracy 0.903663\n",
            "Iter 35, validation loss 2.055920, validation accuracy 0.508490\n",
            "Iter 36, training loss 0.207824, training accuracy 0.925402\n",
            "Iter 36, validation loss 1.898275, validation accuracy 0.522341\n",
            "Iter 37, training loss 0.197540, training accuracy 0.931209\n",
            "Iter 37, validation loss 1.771982, validation accuracy 0.535299\n",
            "Iter 38, training loss 0.209706, training accuracy 0.921977\n",
            "Iter 38, validation loss 1.706660, validation accuracy 0.534406\n",
            "Iter 39, training loss 0.221191, training accuracy 0.915277\n",
            "Iter 39, validation loss 1.689980, validation accuracy 0.539768\n",
            "Iter 40, training loss 0.216106, training accuracy 0.917957\n",
            "Iter 40, validation loss 1.696448, validation accuracy 0.537534\n",
            "Iter 41, training loss 0.196765, training accuracy 0.928529\n",
            "Iter 41, validation loss 1.716885, validation accuracy 0.536640\n",
            "Iter 42, training loss 0.175778, training accuracy 0.939101\n",
            "Iter 42, validation loss 1.754908, validation accuracy 0.541108\n",
            "Iter 43, training loss 0.162574, training accuracy 0.940887\n",
            "Iter 43, validation loss 1.807060, validation accuracy 0.538874\n",
            "Iter 44, training loss 0.158023, training accuracy 0.939547\n",
            "Iter 44, validation loss 1.853196, validation accuracy 0.535746\n",
            "Iter 45, training loss 0.156710, training accuracy 0.937314\n",
            "Iter 45, validation loss 1.870973, validation accuracy 0.531725\n",
            "Iter 46, training loss 0.151905, training accuracy 0.938952\n",
            "Iter 46, validation loss 1.847935, validation accuracy 0.529044\n",
            "Iter 47, training loss 0.143380, training accuracy 0.943717\n",
            "Iter 47, validation loss 1.791123, validation accuracy 0.532618\n",
            "Iter 48, training loss 0.135302, training accuracy 0.947588\n",
            "Iter 48, validation loss 1.720779, validation accuracy 0.536640\n",
            "Iter 49, training loss 0.130138, training accuracy 0.949226\n",
            "Iter 49, validation loss 1.655703, validation accuracy 0.532172\n",
            "Iter 50, training loss 0.126355, training accuracy 0.951757\n",
            "Iter 50, validation loss 1.609047, validation accuracy 0.532618\n",
            "Iter 51, training loss 0.121341, training accuracy 0.954288\n",
            "Iter 51, validation loss 1.587291, validation accuracy 0.537534\n",
            "Iter 52, training loss 0.114952, training accuracy 0.957713\n",
            "Iter 52, validation loss 1.593741, validation accuracy 0.541555\n",
            "Iter 53, training loss 0.109485, training accuracy 0.958160\n",
            "Iter 53, validation loss 1.627568, validation accuracy 0.537980\n",
            "Iter 54, training loss 0.106723, training accuracy 0.958160\n",
            "Iter 54, validation loss 1.676427, validation accuracy 0.539321\n",
            "Iter 55, training loss 0.104461, training accuracy 0.958457\n",
            "Iter 55, validation loss 1.712042, validation accuracy 0.539321\n",
            "Iter 56, training loss 0.100268, training accuracy 0.959053\n",
            "Iter 56, validation loss 1.718472, validation accuracy 0.537087\n",
            "Iter 57, training loss 0.094486, training accuracy 0.961286\n",
            "Iter 57, validation loss 1.698513, validation accuracy 0.537534\n",
            "Iter 58, training loss 0.089547, training accuracy 0.963520\n",
            "Iter 58, validation loss 1.668914, validation accuracy 0.538427\n",
            "Iter 59, training loss 0.086834, training accuracy 0.965009\n",
            "Iter 59, validation loss 1.646333, validation accuracy 0.537087\n",
            "Iter 60, training loss 0.085295, training accuracy 0.965753\n",
            "Iter 60, validation loss 1.638137, validation accuracy 0.533959\n",
            "Iter 61, training loss 0.082806, training accuracy 0.967094\n",
            "Iter 61, validation loss 1.643980, validation accuracy 0.534853\n",
            "Iter 62, training loss 0.078913, training accuracy 0.968285\n",
            "Iter 62, validation loss 1.661281, validation accuracy 0.532618\n",
            "Iter 63, training loss 0.075050, training accuracy 0.969178\n",
            "Iter 63, validation loss 1.688395, validation accuracy 0.538874\n",
            "Iter 64, training loss 0.072375, training accuracy 0.969476\n",
            "Iter 64, validation loss 1.716503, validation accuracy 0.534853\n",
            "Iter 65, training loss 0.070172, training accuracy 0.970071\n",
            "Iter 65, validation loss 1.724849, validation accuracy 0.533959\n",
            "Iter 66, training loss 0.068438, training accuracy 0.970071\n",
            "Iter 66, validation loss 1.711208, validation accuracy 0.534853\n",
            "Iter 67, training loss 0.067096, training accuracy 0.970965\n",
            "Iter 67, validation loss 1.694302, validation accuracy 0.533512\n",
            "Iter 68, training loss 0.064886, training accuracy 0.971560\n",
            "Iter 68, validation loss 1.692052, validation accuracy 0.533959\n",
            "Iter 69, training loss 0.062564, training accuracy 0.972007\n",
            "Iter 69, validation loss 1.710215, validation accuracy 0.535299\n",
            "Iter 70, training loss 0.061418, training accuracy 0.972007\n",
            "Iter 70, validation loss 1.736990, validation accuracy 0.529044\n",
            "Iter 71, training loss 0.060374, training accuracy 0.972007\n",
            "Iter 71, validation loss 1.749341, validation accuracy 0.529491\n",
            "Iter 72, training loss 0.058669, training accuracy 0.972156\n",
            "Iter 72, validation loss 1.742494, validation accuracy 0.528597\n",
            "Iter 73, training loss 0.057128, training accuracy 0.972901\n",
            "Iter 73, validation loss 1.733777, validation accuracy 0.531725\n",
            "Iter 74, training loss 0.056081, training accuracy 0.972901\n",
            "Iter 74, validation loss 1.740622, validation accuracy 0.533065\n",
            "Iter 75, training loss 0.055137, training accuracy 0.972305\n",
            "Iter 75, validation loss 1.760616, validation accuracy 0.537087\n",
            "Iter 76, training loss 0.053816, training accuracy 0.973496\n",
            "Iter 76, validation loss 1.777474, validation accuracy 0.536193\n",
            "Iter 77, training loss 0.052706, training accuracy 0.973794\n",
            "Iter 77, validation loss 1.788019, validation accuracy 0.533512\n",
            "Iter 78, training loss 0.052289, training accuracy 0.974092\n",
            "Iter 78, validation loss 1.802769, validation accuracy 0.531725\n",
            "Iter 79, training loss 0.051520, training accuracy 0.974985\n",
            "Iter 79, validation loss 1.818007, validation accuracy 0.529937\n",
            "Iter 80, training loss 0.050336, training accuracy 0.975283\n",
            "Iter 80, validation loss 1.826133, validation accuracy 0.530384\n",
            "Iter 81, training loss 0.049739, training accuracy 0.975878\n",
            "Iter 81, validation loss 1.828546, validation accuracy 0.531725\n",
            "Iter 82, training loss 0.048912, training accuracy 0.976325\n",
            "Iter 82, validation loss 1.838947, validation accuracy 0.534853\n",
            "Iter 83, training loss 0.047525, training accuracy 0.976325\n",
            "Iter 83, validation loss 1.868797, validation accuracy 0.532618\n",
            "Iter 84, training loss 0.046917, training accuracy 0.977367\n",
            "Iter 84, validation loss 1.900565, validation accuracy 0.531725\n",
            "Iter 85, training loss 0.046068, training accuracy 0.977665\n",
            "Iter 85, validation loss 1.905664, validation accuracy 0.532172\n",
            "Iter 86, training loss 0.045405, training accuracy 0.977219\n",
            "Iter 86, validation loss 1.902179, validation accuracy 0.537980\n",
            "Iter 87, training loss 0.044916, training accuracy 0.977367\n",
            "Iter 87, validation loss 1.915816, validation accuracy 0.536640\n",
            "Iter 88, training loss 0.044460, training accuracy 0.977516\n",
            "Iter 88, validation loss 1.940039, validation accuracy 0.539768\n",
            "Iter 89, training loss 0.044300, training accuracy 0.977367\n",
            "Iter 89, validation loss 1.969167, validation accuracy 0.541555\n",
            "Iter 90, training loss 0.044046, training accuracy 0.977070\n",
            "Iter 90, validation loss 1.966082, validation accuracy 0.547364\n",
            "Iter 91, training loss 0.044517, training accuracy 0.977219\n",
            "Iter 91, validation loss 1.977234, validation accuracy 0.553619\n",
            "Iter 92, training loss 0.044627, training accuracy 0.977219\n",
            "Iter 92, validation loss 2.056926, validation accuracy 0.538427\n",
            "Iter 93, training loss 0.045281, training accuracy 0.976623\n",
            "Iter 93, validation loss 2.055867, validation accuracy 0.546023\n",
            "Iter 94, training loss 0.045685, training accuracy 0.974687\n",
            "Iter 94, validation loss 2.096365, validation accuracy 0.543342\n",
            "Iter 95, training loss 0.049118, training accuracy 0.974241\n",
            "Iter 95, validation loss 2.090587, validation accuracy 0.550938\n",
            "Iter 96, training loss 0.071722, training accuracy 0.958309\n",
            "Iter 96, validation loss 2.469803, validation accuracy 0.521001\n",
            "Iter 97, training loss 0.244465, training accuracy 0.901281\n",
            "Iter 97, validation loss 2.268699, validation accuracy 0.517873\n",
            "Iter 98, training loss 0.545820, training accuracy 0.814473\n",
            "Iter 98, validation loss 4.249236, validation accuracy 0.472744\n",
            "Iter 99, training loss 0.097847, training accuracy 0.952799\n",
            "Iter 99, validation loss 2.116583, validation accuracy 0.545130\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0Q7hrHK5giG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "outputId": "449809d5-9285-4e7a-8950-32dd952b3055"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "epoch=list(range(1,epochs+1))\n",
        "plt.figure()\n",
        "plt.plot(epoch,tr_losses,label='training accuracy')\n",
        "plt.plot(epoch,te_losses,label='validation accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(\"Best epoch number is: \", te_losses.index(max(te_losses)))\n",
        "print(\"The hightest validation accuracy is: \", max(te_losses))\n",
        "print(\"At the same epoch, the training accuracy is: \", tr_losses[te_losses.index(max(te_losses))])"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Best epoch number is:  91\n",
            "The hightest validation accuracy is:  0.5536193\n",
            "At the same epoch, the training accuracy is:  0.97721857\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Icf8yWaBYFuV"
      },
      "source": [
        "###Gradient Boosting Classifier \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zaAUqHcpHW0n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "97a9f6e0-a4e8-4da6-c64b-fbbc8f8cadbf"
      },
      "source": [
        "GBoost = GradientBoostingClassifier(n_estimators=3000,learning_rate=0.05,max_depth=3,max_features='sqrt',\n",
        "                                          min_samples_leaf=15,min_samples_split=10)\n",
        "GBoost.fit(x_train_diag,y_train_diag)\n",
        "scores = cross_val_score(GBoost, x_train_diag, y_train_diag, cv=5)\n",
        "print('Training Accuracy: %0.2f (+/- %0.2f)' % (scores.mean(), scores.std()*2))\n",
        "y_validation_pred = GBoost.predict(x_val_diag)\n",
        "acc = accuracy_score(y_validation_pred,y_val_diag)\n",
        "print('Validation Accuracy: %0.2f (+/- %0.2f)' % (acc.mean(), acc.std()*2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy: 0.84 (+/- 0.07)\n",
            "Validation Accuracy: 0.73 (+/- 0.00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8adZIYmLh6cu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "4e336074-b395-47ef-d323-7a75cd467801"
      },
      "source": [
        "cn_cls,mci_cls,ad_cls,cn_pred,mci_pred,ad_pred = transform(y_val_diag,y_validation_pred)\n",
        "metrics('CN_Diag',GBoost,x_val,cn_cls,cn_pred)\n",
        "print('*'*30)\n",
        "metrics('MCI_Diag',GBoost,x_val,mci_cls,mci_pred)\n",
        "print('*'*30)\n",
        "metrics('AD_Diag',GBoost,x_val,ad_cls,ad_pred)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CN_Diag Accuracy: 84.67% \n",
            "CN_Diag Precision: 84.20% \n",
            "CN_Diag Recall: 72.36% \n",
            "CN_Diag AUC: 26.11% \n",
            "******************************\n",
            "MCI_Diag Accuracy: 77.52% \n",
            "MCI_Diag Precision: 74.89% \n",
            "MCI_Diag Recall: 72.66% \n",
            "MCI_Diag AUC: 79.98% \n",
            "******************************\n",
            "AD_Diag Accuracy: 77.66% \n",
            "AD_Diag Precision: 44.21% \n",
            "AD_Diag Recall: 59.12% \n",
            "AD_Diag AUC: 38.55% \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwzbCMa25gig",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}